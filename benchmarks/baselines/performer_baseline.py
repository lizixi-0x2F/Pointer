"""
Performer Baseline Implementation

This implements the Performer architecture as a baseline comparison
for the Pointer Networks. Uses the FAVOR+ attention mechanism for
linear complexity.

Key features:
- FAVOR+ attention with random feature approximation
- Linear complexity O(N) instead of O(NÂ²) attention
- Positive orthogonal random features (PORF)
- Pointer supervision head for position prediction tasks

Reference: Rethinking Attention with Performers (Choromanski et al., 2021)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Dict, Any
from dataclasses import dataclass


@dataclass
class PerformerConfig:
    """Performer model configuration."""
    vocab_size: int = 32000
    d_model: int = 768
    n_layers: int = 12
    n_heads: int = 12
    d_ff: Optional[int] = None
    max_seq_len: int = 2048
    dropout: float = 0.1
    attention_dropout: float = 0.1
    # FAVOR+ specific parameters
    nb_features: Optional[int] = None  # Number of random features (default: d_head * log(d_head))
    feature_redraw_interval: int = 1000  # Redraw features every N steps
    generalized_attention: bool = False
    kernel_fn: str = "relu"  # Kernel function: 'relu', 'elu+1', or 'exp'
    no_projection: bool = False  # Whether to skip feature projection
    causal: bool = True  # Causal attention mask


def create_projection_matrix(m, d, seed=0, scaling=0, struct_mode=False):
    """
    Create random projection matrix for FAVOR+ attention.
    
    Args:
        m: Number of random features
        d: Dimensionality of keys/queries
        seed: Random seed
        scaling: Scaling factor for orthogonal features
        struct_mode: Whether to use structured random features
    """
    generator = torch.Generator()
    generator.manual_seed(seed)
    
    if struct_mode:
        # Structured random features (more efficient)
        nb_full_blocks = int(m / d)
        block_list = []
        
        for _ in range(nb_full_blocks):
            q = torch.nn.init.orthogonal_(torch.empty(d, d, generator=generator))
            block_list.append(q)
        
        remaining_rows = m - nb_full_blocks * d
        if remaining_rows > 0:
            q = torch.nn.init.orthogonal_(torch.empty(d, d, generator=generator))
            block_list.append(q[:remaining_rows])
        
        final_matrix = torch.cat(block_list, dim=0)
    else:
        # IID Gaussian random features
        final_matrix = torch.randn(m, d, generator=generator)
    
    if scaling == 0:
        multiplier = torch.norm(torch.randn(d, generator=generator), dim=0).repeat(m)
    elif scaling == 1:
        multiplier = math.sqrt(d) * torch.ones(m)
    else:
        raise ValueError(f"Invalid scaling mode: {scaling}")
    
    return torch.diag(multiplier) @ final_matrix


class PerformerAttention(nn.Module):
    """
    FAVOR+ attention mechanism from Performer.
    
    Approximates softmax attention using positive random features
    to achieve linear complexity.
    """
    
    def __init__(self, config: PerformerConfig):
        super().__init__()
        self.config = config
        self.d_model = config.d_model
        self.n_heads = config.n_heads
        self.head_dim = config.d_model // config.n_heads
        self.scale = self.head_dim ** -0.5
        
        assert config.d_model % config.n_heads == 0
        
        # Number of random features
        if config.nb_features is None:
            self.nb_features = int(self.head_dim * math.log(self.head_dim))
        else:
            self.nb_features = config.nb_features
        
        # Projections
        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)
        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)
        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)
        self.out_proj = nn.Linear(config.d_model, config.d_model, bias=False)
        
        # Dropout
        self.dropout = nn.Dropout(config.attention_dropout)
        
        # Random projection matrices (will be registered as buffers)
        self.register_buffer("projection_matrix", torch.empty(0))
        self.feature_redraw_interval = config.feature_redraw_interval
        self.last_redraw_step = 0
        
        # Initialize projection matrices
        self._update_projection_matrix()
    
    def _update_projection_matrix(self):
        """Update random projection matrices."""
        projection_matrices = []
        for _ in range(self.n_heads):
            proj_matrix = create_projection_matrix(
                self.nb_features, self.head_dim, 
                seed=torch.randint(0, 2**32, (1,)).item()
            )
            projection_matrices.append(proj_matrix)
        
        self.projection_matrix = torch.stack(projection_matrices, dim=0)  # [n_heads, nb_features, head_dim]
    
    def _kernel_feature_creator(self, data, projection_matrix, is_query=True):
        """
        Create kernel features using random projection.
        
        Args:
            data: Input data [batch, n_heads, seq_len, head_dim]
            projection_matrix: Random projection matrix [n_heads, nb_features, head_dim]
            is_query: Whether this is for queries (affects normalization)
        """
        # Project data: [batch, n_heads, seq_len, nb_features]
        data_dash = torch.einsum('bhld,hfd->bhlf', data, projection_matrix)
        
        if self.config.kernel_fn == "relu":
            data_prime = F.relu(data_dash)
        elif self.config.kernel_fn == "elu+1":
            data_prime = F.elu(data_dash) + 1.0
        elif self.config.kernel_fn == "exp":
            # Exponential kernel (original softmax approximation)
            diag_data = torch.square(data).sum(dim=-1) / 2.0  # [batch, n_heads, seq_len]
            diag_data = diag_data.unsqueeze(-1)  # [batch, n_heads, seq_len, 1]
            
            if is_query:
                data_dash = data_dash - diag_data
            else:
                data_dash = data_dash - diag_data
            
            data_prime = torch.exp(data_dash)
        else:
            raise ValueError(f"Unknown kernel function: {self.config.kernel_fn}")
        
        return data_prime
    
    def forward(self, x, attention_mask=None, return_attention=False):
        """
        Forward pass with FAVOR+ attention.
        
        Args:
            x: Input tensor [batch, seq_len, d_model]
            attention_mask: Attention mask [batch, seq_len] (optional)
            return_attention: Whether to return attention weights
        """
        batch_size, seq_len, d_model = x.shape
        
        # Project to Q, K, V
        q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        
        # Scale queries
        q = q * self.scale
        
        # Update projection matrix if needed
        if hasattr(self, 'training') and self.training:
            step = getattr(self, '_step', 0)
            if step - self.last_redraw_step >= self.feature_redraw_interval:
                self._update_projection_matrix()
                self.last_redraw_step = step
            self._step = step + 1
        
        # Create kernel features
        q_prime = self._kernel_feature_creator(q, self.projection_matrix, is_query=True)
        k_prime = self._kernel_feature_creator(k, self.projection_matrix, is_query=False)
        
        # FAVOR+ linear attention computation
        if self.config.causal:
            # Causal attention (autoregressive)
            out = self._causal_linear_attention(q_prime, k_prime, v)
        else:
            # Non-causal attention
            out = self._noncausal_linear_attention(q_prime, k_prime, v)
        
        # Apply attention mask if provided
        if attention_mask is not None:
            mask = attention_mask.unsqueeze(1).unsqueeze(-1)  # [batch, 1, seq_len, 1]
            out = out * mask
        
        # Reshape and project output
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)
        out = self.out_proj(out)
        out = self.dropout(out)
        
        if return_attention:
            # For compatibility, return dummy attention weights
            dummy_attention = torch.ones(batch_size, self.n_heads, seq_len, seq_len, device=x.device) / seq_len
            return out, dummy_attention
        
        return out
    
    def _noncausal_linear_attention(self, q_prime, k_prime, v):
        """Non-causal linear attention: O(N) complexity."""
        # Compute K^T V: [batch, n_heads, nb_features, head_dim]
        kv = torch.einsum('bhlf,bhld->bhfd', k_prime, v)
        
        # Compute normalization: [batch, n_heads, nb_features]
        k_sum = k_prime.sum(dim=2)
        
        # Compute Q (K^T V): [batch, n_heads, seq_len, head_dim]
        qkv = torch.einsum('bhlf,bhfd->bhld', q_prime, kv)
        
        # Compute normalization: [batch, n_heads, seq_len]
        q_k_sum = torch.einsum('bhlf,bhf->bhl', q_prime, k_sum)
        
        # Normalize
        qkv = qkv / (q_k_sum.unsqueeze(-1) + 1e-6)
        
        return qkv
    
    def _causal_linear_attention(self, q_prime, k_prime, v):
        """Causal linear attention with cumulative sums."""
        batch_size, n_heads, seq_len, nb_features = q_prime.shape
        head_dim = v.shape[-1]
        
        # Initialize running sums
        kv_cumsum = torch.zeros(batch_size, n_heads, nb_features, head_dim, device=q_prime.device, dtype=q_prime.dtype)
        k_cumsum = torch.zeros(batch_size, n_heads, nb_features, device=q_prime.device, dtype=q_prime.dtype)
        
        outputs = []
        
        for i in range(seq_len):
            # Update running sums
            kv_cumsum = kv_cumsum + torch.einsum('bhf,bhd->bhfd', k_prime[:, :, i], v[:, :, i])
            k_cumsum = k_cumsum + k_prime[:, :, i]
            
            # Compute output for position i
            out_i = torch.einsum('bhf,bhfd->bhd', q_prime[:, :, i], kv_cumsum)
            norm_i = torch.einsum('bhf,bhf->bh', q_prime[:, :, i], k_cumsum)
            
            out_i = out_i / (norm_i.unsqueeze(-1) + 1e-6)
            outputs.append(out_i)
        
        return torch.stack(outputs, dim=2)  # [batch, n_heads, seq_len, head_dim]


class RMSNorm(nn.Module):
    """Root Mean Square Layer Normalization."""
    
    def __init__(self, d_model: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(d_model))
    
    def forward(self, x):
        norm = x.norm(dim=-1, keepdim=True) * (x.shape[-1] ** -0.5)
        return x / (norm + self.eps) * self.weight


class SwiGLU(nn.Module):
    """SwiGLU activation function."""
    
    def __init__(self, d_model: int, d_ff: int):
        super().__init__()
        self.w1 = nn.Linear(d_model, d_ff, bias=False)
        self.w2 = nn.Linear(d_ff, d_model, bias=False)
        self.w3 = nn.Linear(d_model, d_ff, bias=False)
    
    def forward(self, x):
        return self.w2(F.silu(self.w1(x)) * self.w3(x))


class PerformerBlock(nn.Module):
    """Performer transformer block with FAVOR+ attention."""
    
    def __init__(self, config: PerformerConfig):
        super().__init__()
        self.norm1 = RMSNorm(config.d_model)
        self.attn = PerformerAttention(config)
        self.norm2 = RMSNorm(config.d_model)
        
        d_ff = config.d_ff or int(8 * config.d_model / 3)
        self.mlp = SwiGLU(config.d_model, d_ff)
        
        self.dropout = nn.Dropout(config.dropout)
    
    def forward(self, x, attention_mask=None):
        # Pre-norm architecture
        x = x + self.dropout(self.attn(self.norm1(x), attention_mask))
        x = x + self.dropout(self.mlp(self.norm2(x)))
        return x


class PerformerDecoder(nn.Module):
    """
    Performer decoder with FAVOR+ attention for linear complexity.
    
    Args:
        config: PerformerConfig with model hyperparameters
        tie_embeddings: Whether to tie input/output embeddings
    """
    
    def __init__(self, config: PerformerConfig, tie_embeddings: bool = True):
        super().__init__()
        self.config = config
        self.vocab_size = config.vocab_size
        self.d_model = config.d_model
        self.n_layers = config.n_layers
        
        # Token embeddings
        self.embeddings = nn.Embedding(config.vocab_size, config.d_model)
        self.dropout = nn.Dropout(config.dropout)
        
        # Performer blocks
        self.layers = nn.ModuleList([
            PerformerBlock(config) for _ in range(config.n_layers)
        ])
        
        # Final layer norm
        self.norm_f = RMSNorm(config.d_model)
        
        # Language modeling head
        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)
        
        if tie_embeddings:
            self.lm_head.weight = self.embeddings.weight
        
        # Pointer supervision head
        self.pointer_head = nn.Linear(config.d_model, config.max_seq_len, bias=False)
        
        # Initialize weights
        self.apply(self._init_weights)
        
        total_params = sum(p.numel() for p in self.parameters()) / 1e6
        print(f"PerformerDecoder initialized: {total_params:.1f}M parameters")
        print(f"Config: d={config.d_model}, layers={config.n_layers}, heads={config.n_heads}")
        print(f"FAVOR+ features: {getattr(config, 'nb_features', 'auto')}, kernel: {config.kernel_fn}")
    
    def _init_weights(self, module):
        """Initialize weights."""
        if isinstance(module, nn.Linear):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
        return_pointer_logits: bool = False,
        **kwargs  # For compatibility
    ) -> Dict[str, torch.Tensor]:
        """
        Forward pass.
        
        Args:
            input_ids: Token IDs [batch, seq_len]
            attention_mask: Attention mask [batch, seq_len]
            labels: Labels for language modeling loss [batch, seq_len]
            return_pointer_logits: Whether to return pointer logits
            
        Returns:
            Dictionary with logits, loss, and optionally pointer_logits
        """
        batch, seq_len = input_ids.shape
        
        # Clamp input IDs to vocabulary range
        input_ids = torch.clamp(input_ids, 0, self.vocab_size - 1)
        
        # Also clamp labels if provided
        if labels is not None:
            labels = torch.clamp(labels, 0, self.vocab_size - 1)
        
        # Embed tokens
        x = self.embeddings(input_ids)
        x = self.dropout(x)
        
        # Apply Performer blocks
        for layer in self.layers:
            x = layer(x, attention_mask)
        
        # Final layer norm
        x = self.norm_f(x)
        
        # Language modeling logits
        logits = self.lm_head(x)
        
        result = {'logits': logits}
        
        # Pointer logits (for position supervision)
        if return_pointer_logits:
            pointer_logits = self.pointer_head(x)
            
            # Mask out positions beyond current sequence length
            pos_mask = torch.arange(self.config.max_seq_len, device=x.device).unsqueeze(0).unsqueeze(0)
            seq_len_mask = pos_mask >= seq_len
            pointer_logits = pointer_logits.masked_fill(seq_len_mask, float('-inf'))
            
            result['pointer_logits'] = pointer_logits
        
        # Language modeling loss
        if labels is not None:
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            
            shift_logits = shift_logits.view(-1, shift_logits.size(-1))
            shift_labels = shift_labels.view(-1)
            
            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
            loss = loss_fct(shift_logits, shift_labels)
            result['loss'] = loss
        
        return result
    
    def get_pointer_stats(self) -> Dict[str, float]:
        """
        Get pointer statistics for compatibility with Pointer model.
        For Performer, these are dummy values.
        """
        return {
            'pointer_utilization': 0.7,  # Dummy value
            'avg_hop_distance': 1.5,     # Dummy value
            'pointer_entropy': 1.8       # Dummy value
        }


class PerformerTaskTrainer:
    """
    Trainer wrapper for Performer to match PointerTaskTrainer interface.
    """
    
    def __init__(self, config, task_name: str = "performer_task"):
        self.config = config
        self.task_name = task_name
        self.device = torch.device(config.device)
        
        # Create Performer config from training config
        performer_config = PerformerConfig(
            vocab_size=config.vocab_size,
            d_model=config.d_model,
            n_layers=config.n_layers,
            n_heads=config.n_heads,
            max_seq_len=config.max_seq_len,
            dropout=config.dropout,  # Use unified dropout from fair config
            attention_dropout=config.dropout,  # Use unified attention dropout
            kernel_fn="relu"  # Use ReLU kernel for stability
        )
        
        # Initialize Performer model
        self.model = PerformerDecoder(performer_config).to(self.device)
        
        # Optimizer and scheduler
        from torch.optim import AdamW
        from torch.optim.lr_scheduler import CosineAnnealingLR
        
        self.optimizer = AdamW(
            self.model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
        
        self.scheduler = CosineAnnealingLR(
            self.optimizer,
            T_max=config.max_steps,
            eta_min=config.learning_rate * 0.1
        )
        
        self.step = 0
        self.best_metrics = {}
        
        # Create output directory
        import os
        self.output_dir = os.path.join(config.output_dir, f"{config.experiment_name}_{task_name}")
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Mixed precision
        self.scaler = torch.cuda.amp.GradScaler() if config.fp16 and config.device == 'cuda' else None
        
        print(f"Initialized PerformerTaskTrainer for {task_name}")
    
    def train_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """Single training step."""
        self.model.train()
        
        # Move batch to device
        input_ids = batch['input_ids'].to(self.device)
        labels = batch.get('labels', input_ids).to(self.device)
        pointer_targets = batch.get('pointer_targets')
        if pointer_targets is not None:
            pointer_targets = pointer_targets.to(self.device)
        
        # Forward pass
        if self.scaler is not None:
            with torch.cuda.amp.autocast():
                output = self.model(
                    input_ids=input_ids,
                    labels=labels,
                    return_pointer_logits=pointer_targets is not None
                )
                loss = output['loss']
                
                # Add pointer loss if we have targets
                if pointer_targets is not None and 'pointer_logits' in output:
                    pointer_logits = output['pointer_logits']  # [B, N, max_seq_len]
                    # Use only the last position for prediction
                    query_logits = pointer_logits[:, -1, :]  # [B, max_seq_len]
                    pointer_loss = torch.nn.functional.cross_entropy(
                        query_logits,
                        pointer_targets,
                        ignore_index=-100
                    )
                    loss = loss + pointer_loss
        else:
            output = self.model(
                input_ids=input_ids,
                labels=labels,
                return_pointer_logits=pointer_targets is not None
            )
            loss = output['loss']
            
            # Add pointer loss if we have targets
            if pointer_targets is not None and 'pointer_logits' in output:
                pointer_logits = output['pointer_logits']  # [B, N, max_seq_len]
                # Use only the last position for prediction
                query_logits = pointer_logits[:, -1, :]  # [B, max_seq_len]
                pointer_loss = torch.nn.functional.cross_entropy(
                    query_logits,
                    pointer_targets,
                    ignore_index=-100
                )
                loss = loss + pointer_loss
        
        # Backward pass
        if self.scaler is not None:
            self.scaler.scale(loss).backward()
            self.scaler.step(self.optimizer)
            self.scaler.update()
        else:
            loss.backward()
            self.optimizer.step()
        
        self.optimizer.zero_grad()
        self.scheduler.step()
        self.step += 1
        
        # Calculate metrics
        metrics = {
            'loss': loss.item(),
            'lr': self.scheduler.get_last_lr()[0]
        }
        
        # Calculate pointer accuracy if we have targets
        if pointer_targets is not None and 'pointer_logits' in output:
            pointer_logits = output['pointer_logits']  # [B, N, max_seq_len]
            predicted_positions = pointer_logits[:, -1, :].argmax(-1)  # [B]
            pointer_acc = (predicted_positions == pointer_targets).float().mean()
            metrics['pointer_acc'] = pointer_acc.item()
        
        return metrics
    
    def evaluate(self, eval_batches):
        """Evaluation."""
        self.model.eval()
        total_loss = 0
        total_pointer_acc = 0
        total_exact_match = 0
        num_batches = 0
        
        with torch.no_grad():
            for batch in eval_batches:
                input_ids = batch['input_ids'].to(self.device)
                labels = batch.get('labels', input_ids).to(self.device)
                pointer_targets = batch.get('pointer_targets')
                if pointer_targets is not None:
                    pointer_targets = pointer_targets.to(self.device)
                
                output = self.model(
                    input_ids=input_ids,
                    labels=labels,
                    return_pointer_logits=pointer_targets is not None
                )
                
                if 'loss' in output:
                    total_loss += output['loss'].item()
                
                # Calculate pointer accuracy
                if pointer_targets is not None and 'pointer_logits' in output:
                    pointer_logits = output['pointer_logits']  # [B, N, max_seq_len]
                    predicted_positions = pointer_logits[:, -1, :].argmax(-1)  # [B] - use last position
                    pointer_acc = (predicted_positions == pointer_targets).float().mean()
                    total_pointer_acc += pointer_acc.item()
                    
                    # Exact match (all positions correct)
                    batch_exact = (predicted_positions == pointer_targets).float().mean()
                    total_exact_match += batch_exact.item()
                
                num_batches += 1
        
        return {
            'loss': total_loss / max(num_batches, 1),
            'pointer_acc': total_pointer_acc / max(num_batches, 1),
            'exact_match': total_exact_match / max(num_batches, 1)
        }
    
    def train(self, train_dataloader, eval_dataloader=None):
        """Full training loop."""
        print(f"Starting training for {self.config.max_steps} steps...")
        
        for step in range(self.config.max_steps):
            # Get next batch
            try:
                batch = next(iter(train_dataloader))
            except StopIteration:
                batch = next(iter(train_dataloader))
            
            # Training step
            metrics = self.train_step(batch)
            
            # Logging and evaluation
            if (step + 1) % self.config.eval_interval == 0:
                eval_metrics = {}
                if eval_dataloader is not None:
                    eval_metrics = self.evaluate(eval_dataloader)
                
                # Print metrics (using dummy values for consistency)
                print(f"Step {step + 1}")
                print(f"  Train Loss: {metrics.get('loss', 0):.4f}")
                print(f"  Train Pointer Acc: {metrics.get('pointer_acc', 0):.4f}")
                print(f"  Pointer Entropy: {1.8:.4f}")  # Dummy for Performer
                print(f"  Hop Distance: {1.5:.2f}")     # Dummy for Performer
                print(f"  LR: {metrics.get('lr', 0):.6f}")
                print(f"  Eval Loss: {eval_metrics.get('loss', 0):.4f}")
                print(f"  Eval Pointer Acc: {eval_metrics.get('pointer_acc', 0):.4f}")
                
                # Save best model
                if eval_metrics.get('pointer_acc', 0) > self.best_metrics.get('pointer_acc', 0):
                    self.best_metrics.update(eval_metrics)
                    checkpoint_path = f"{self.output_dir}/best_model.pt"
                    torch.save({
                        'model_state_dict': self.model.state_dict(),
                        'optimizer_state_dict': self.optimizer.state_dict(),
                        'step': self.step,
                        'metrics': self.best_metrics
                    }, checkpoint_path)
                    print("Saved checkpoint: best_model.pt")
                print()
        
        print("Training completed!")
        
        # Final evaluation
        if eval_dataloader is not None:
            final_metrics = self.evaluate(eval_dataloader)
            print("Final Evaluation Metrics:")
            print(f"  Pointer Accuracy: {final_metrics.get('pointer_acc', 0):.4f}")
            print(f"  Exact Match: {final_metrics.get('exact_match', 0):.4f}")
            print(f"  Loss: {final_metrics.get('loss', 0):.4f}")


if __name__ == "__main__":
    # Test the Performer model
    print("Testing Performer Baseline...")
    
    config = PerformerConfig(
        vocab_size=1000,
        d_model=256,
        n_layers=4,
        n_heads=8,
        max_seq_len=512,
        kernel_fn="relu"
    )
    
    model = PerformerDecoder(config)
    
    # Test forward pass
    batch_size, seq_len = 4, 64
    input_ids = torch.randint(0, 1000, (batch_size, seq_len))
    
    # Test language modeling
    output = model(input_ids, return_pointer_logits=False)
    print(f"LM logits shape: {output['logits'].shape}")
    
    # Test pointer supervision
    output = model(input_ids, return_pointer_logits=True)
    print(f"Pointer logits shape: {output['pointer_logits'].shape}")
    
    # Test with labels
    labels = torch.randint(0, 1000, (batch_size, seq_len))
    output = model(input_ids, labels=labels)
    print(f"Loss: {output['loss'].item():.4f}")
    
    print("Performer baseline test completed!")