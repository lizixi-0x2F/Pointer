import os
import sys
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
root_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, root_path)

# ç›´æ¥å¯¼å…¥æ¨¡å‹
from src.model.pointer_model import PointerDecoder
from benchmarks.baselines import TransformerBaseline  # åªå¯¼å…¥transformer
from tqdm import tqdm
import datasets
import argparse

def load_dataset(dataset_name):
    """åŠ è½½listops-32æˆ–wikitext-2æ•°æ®é›†"""
    if dataset_name == "listops-32":
        # ä»æœ¬åœ°è·¯å¾„åŠ è½½listopsæ•°æ®é›†
        local_path = "/Volumes/oz/pointer/listops-32"
        train = datasets.load_dataset(local_path, split="train")
        val = datasets.load_dataset(local_path, split="validation")
        test = datasets.load_dataset(local_path, split="test")
        
        # ç®€å•é¢„å¤„ç†
        vocab_size = 32
        max_len = 512  # å‡å°‘åºåˆ—é•¿åº¦æé«˜æ€§èƒ½
        
        def preprocess(batch, max_len=512):
            # æ£€æŸ¥å¹¶é€‚é…ä¸åŒå­—æ®µå
            if "tokens" in batch:
                tokens = batch["tokens"]
            elif "input_ids" in batch:
                tokens = batch["input_ids"]
            elif "Source" in batch:
                # å¤„ç†listops-32çš„ç‰¹æ®Šæ ¼å¼
                import re
                tokens = [int(x) for x in re.findall(r'\d+', batch["Source"])]
            else:
                raise ValueError(f"Invalid batch format. Expected 'tokens', 'input_ids' or 'Source', got: {batch.keys()}")
            
            # ç¡®ä¿tokensæ˜¯åˆ—è¡¨ä¸”é•¿åº¦ä¸è¶…è¿‡max_len
            tokens = tokens[:max_len] if isinstance(tokens, (list, tuple)) else [tokens]
            
            # å¤„ç†æ ‡ç­¾å­—æ®µ
            if "label" in batch:
                label = batch["label"]
            elif "labels" in batch:
                label = batch["labels"]
            elif "Target" in batch:
                label = batch["Target"]
            else:
                label = 0  # é»˜è®¤0å¦‚æœæ— æ ‡ç­¾
            
            # ç¡®ä¿tokensæ˜¯åˆ—è¡¨æˆ–æ•°ç»„
            if isinstance(tokens, (list, tuple)) or (hasattr(tokens, "__array__")):
                pass  # æœ‰æ•ˆæ ¼å¼
            else:
                raise ValueError(f"Invalid tokens format. Expected list/array, got: {type(tokens)}")
            
            # Paddingå¤„ç†
            original_len = len(tokens)
            pad_len = max_len - len(tokens)
            if pad_len > 0:
                tokens = tokens + [0] * pad_len  # ç”¨0å¡«å……
            
            # ç¡®ä¿æ‰€æœ‰tensoréƒ½æ˜¯å›ºå®šé•¿åº¦
            tokens = tokens[:max_len]  # æˆªæ–­åˆ°max_len
            
            return {
                "input_ids": torch.tensor(tokens, dtype=torch.long),
                "labels": torch.tensor(label, dtype=torch.long),
                "attention_mask": torch.tensor([1]*min(original_len, max_len) + [0]*max(0, max_len-original_len), dtype=torch.long)
            }
            
    elif dataset_name == "wikitext-2":
        # ä»æœ¬åœ°è·¯å¾„åŠ è½½wikitext-2æ•°æ®é›†
        local_path = "/Volumes/oz/pointer/wikitext-2"
        train = datasets.load_dataset(local_path, split="train")
        val = datasets.load_dataset(local_path, split="validation")
        test = datasets.load_dataset(local_path, split="test")
        
        # ç®€å•é¢„å¤„ç† - ä½¿ç”¨å­—ç¬¦çº§tokenization
        vocab_size = 256  # ä½¿ç”¨ASCIIå­—ç¬¦é›†
        max_len = 512  # å‡å°‘åºåˆ—é•¿åº¦æé«˜æ€§èƒ½
        
        def preprocess(batch, max_len=512):
            # æ£€æŸ¥å¹¶é€‚é…ä¸åŒå­—æ®µå
            if "text" in batch:
                text = batch["text"]
            elif "input_ids" in batch:
                text = batch["input_ids"] 
            else:
                raise ValueError(f"Invalid batch format. Expected 'text' or 'input_ids', got: {batch.keys()}")
            
            # å¤„ç†æ–‡æœ¬å­—ç¬¦ä¸² - è½¬æ¢ä¸ºå­—ç¬¦çº§token
            if isinstance(text, str):
                # å­—ç¬¦çº§tokenization: æ¯ä¸ªå­—ç¬¦è½¬æ¢ä¸ºASCIIç 
                tokens = [min(ord(c), 255) for c in text[:max_len]]  # é™åˆ¶åœ¨0-255èŒƒå›´
            elif isinstance(text, (list, tuple)):
                tokens = text[:max_len]
            else:
                raise ValueError(f"Invalid text format. Expected str or list, got: {type(text)}")
            
            # Paddingå¤„ç†
            original_len = len(tokens)
            pad_len = max_len - len(tokens)
            if pad_len > 0:
                tokens = tokens + [0] * pad_len  # ç”¨0å¡«å……
            
            # ç¡®ä¿æ‰€æœ‰tensoréƒ½æ˜¯å›ºå®šé•¿åº¦
            tokens = tokens[:max_len]  # æˆªæ–­åˆ°max_len
            
            return {
                "input_ids": torch.tensor(tokens, dtype=torch.long),
                "labels": torch.tensor(tokens, dtype=torch.long),  # è¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼šè¾“å…¥=æ ‡ç­¾
                "attention_mask": torch.tensor([1]*min(original_len, max_len) + [0]*max(0, max_len-original_len), dtype=torch.long)
            }
    
    else:
        raise ValueError(f"Unknown dataset: {dataset_name}")
    
    return train, val, test, vocab_size, max_len, preprocess

def train_model(model, train_loader, val_loader, epochs=1, lr=0.003, return_metrics=False, max_steps=None):
    """è®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼° - ä½¿ç”¨å›°æƒ‘åº¦è€Œéå‡†ç¡®ç‡"""
    if torch.backends.mps.is_available():
        device = "mps"
    elif torch.cuda.is_available():
        device = "cuda"
    else:
        device = "cpu"
    
    print(f"Using device: {device}")
    model = model.to(device)
    
    # è·Ÿè¸ªè®­ç»ƒæŒ‡æ ‡
    training_metrics = {
        'train_losses': [],
        'train_perplexities': [],  # ğŸ¯ æ”¹ä¸ºå›°æƒ‘åº¦
        'val_losses': [],
        'val_perplexities': [],   # ğŸ¯ æ”¹ä¸ºå›°æƒ‘åº¦  
        'pointer_stats': []
    }
    
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss(ignore_index=0)  # å¿½ç•¥padding
    
    if max_steps is not None:
        print(f"Training for {max_steps} steps (1 epoch)")
        epochs = 1  # å¼ºåˆ¶ä½¿ç”¨1ä¸ªepoch
    else:
        print(f"Training for {epochs} epochs")
    
    global_step = 0
    
    for epoch in range(epochs):
        model.train()
        total_loss = 0.0
        total_tokens = 0  # ğŸ¯ è®¡ç®—å›°æƒ‘åº¦éœ€è¦tokenæ•°é‡
        num_batches = 0
        
        if max_steps is not None:
            # ä½¿ç”¨æ­¥æ•°é™åˆ¶çš„è¿›åº¦æ¡
            progress_bar = tqdm(total=max_steps, desc=f"Training Steps")
            data_iter = iter(train_loader)
            
            for step in range(max_steps):
                try:
                    batch = next(data_iter)
                except StopIteration:
                    print(f"\nDataLoader exhausted at step {global_step}, stopping training...")
                    break
                    
                # ç»Ÿä¸€çš„è®­ç»ƒé€»è¾‘
                inputs = batch["input_ids"].to(device)
                labels = batch["labels"].to(device)
                attention_mask = batch.get("attention_mask", None)
                if attention_mask is not None:
                    attention_mask = attention_mask.to(device)
                
                outputs = model(inputs, attention_mask=attention_mask, labels=labels)
                logits = outputs["logits"]
                
                # æ£€æŸ¥ä»»åŠ¡ç±»å‹ï¼šåˆ†ç±» vs è¯­è¨€å»ºæ¨¡
                if labels.dim() == 1:  # åˆ†ç±»ä»»åŠ¡ï¼šlabelsæ˜¯[B]
                    # logitsåº”è¯¥æ˜¯[B, num_classes]
                    if logits.dim() == 3:  # å¦‚æœæ˜¯[B, N, vocab_size]ï¼Œå–ç¬¬ä¸€ä¸ªtoken
                        logits_flat = logits[:, 0, :]  # [B, vocab_size]
                        labels_flat = labels
                    else:
                        logits_flat = logits
                        labels_flat = labels
                    loss = criterion(logits_flat, labels_flat)
                    
                    # åˆ†ç±»ä»»åŠ¡tokenæ•°é‡å¤„ç†
                    with torch.no_grad():
                        valid_tokens = labels_flat.size(0)
                        total_tokens += valid_tokens
                        total_loss += loss.item() * valid_tokens
                        
                else:  # è¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼šlabelsæ˜¯[B, N]
                    if logits.dim() == 3:  # [B, N, vocab_size]
                        logits_flat = logits.view(-1, logits.size(-1))
                        labels_flat = labels.view(-1)
                    else:
                        logits_flat = logits
                        labels_flat = labels
                    loss = criterion(logits_flat, labels_flat)
                    
                    # ğŸ¯ è®¡ç®—å›°æƒ‘åº¦ç›¸å…³æŒ‡æ ‡ - åªè®¡ç®—épadding token
                    with torch.no_grad():
                        valid_tokens = (labels_flat != 0).sum().item()
                        if valid_tokens > 0:
                            total_tokens += valid_tokens
                            total_loss += loss.item() * valid_tokens  # åŠ æƒloss
                
                optimizer.zero_grad()
                loss.backward()
                
                # è¯Šæ–­æ¢¯åº¦ä¿¡æ¯ï¼ˆæ¯50æ­¥æ‰“å°ä¸€æ¬¡ï¼‰
                if global_step % 50 == 0:
                    total_grad_norm = 0
                    param_count = 0
                    for name, param in model.named_parameters():
                        if param.grad is not None:
                            grad_norm = param.grad.norm().item()
                            total_grad_norm += grad_norm
                            param_count += 1
                    avg_grad_norm = total_grad_norm / max(param_count, 1)
                    print(f"  Step {global_step}: avg_grad_norm={avg_grad_norm:.6f}, lr={optimizer.param_groups[0]['lr']:.6f}")
                
                optimizer.step()
                
                num_batches += 1
                global_step += 1
                
                # ğŸ¯ è®¡ç®—å½“å‰å›°æƒ‘åº¦ç”¨äºæ˜¾ç¤º
                if total_tokens > 0:
                    avg_loss = total_loss / total_tokens
                    current_perplexity = torch.exp(torch.tensor(avg_loss)).item()
                else:
                    current_perplexity = float('inf')
                
                # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
                progress_bar.update(1)
                progress_bar.set_postfix({
                    'Step': f"{global_step}/{max_steps}",
                    'Loss': f"{loss.item():.4f}",
                    'PPL': f"{current_perplexity:.2f}"  # ğŸ¯ æ˜¾ç¤ºå›°æƒ‘åº¦è€Œéå‡†ç¡®ç‡
                })
                
        else:
            # ä½¿ç”¨æ ‡å‡†çš„epochè¿›åº¦æ¡
            progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}")
            
            for batch in progress_bar:
                inputs = batch["input_ids"].to(device)
                labels = batch["labels"].to(device)
                attention_mask = batch.get("attention_mask", None)
                if attention_mask is not None:
                    attention_mask = attention_mask.to(device)
                
                outputs = model(inputs, attention_mask=attention_mask, labels=labels)
                logits = outputs["logits"]
                
                # æ£€æŸ¥ä»»åŠ¡ç±»å‹ï¼šåˆ†ç±» vs è¯­è¨€å»ºæ¨¡
                if labels.dim() == 1:  # åˆ†ç±»ä»»åŠ¡ï¼šlabelsæ˜¯[B]
                    # logitsåº”è¯¥æ˜¯[B, num_classes]
                    if logits.dim() == 3:  # å¦‚æœæ˜¯[B, N, vocab_size]ï¼Œå–ç¬¬ä¸€ä¸ªtoken
                        logits_flat = logits[:, 0, :]  # [B, vocab_size]
                        labels_flat = labels
                    else:
                        logits_flat = logits
                        labels_flat = labels
                    loss = criterion(logits_flat, labels_flat)
                    
                    # åˆ†ç±»ä»»åŠ¡tokenæ•°é‡å¤„ç†
                    with torch.no_grad():
                        valid_tokens = labels_flat.size(0)
                        total_tokens += valid_tokens
                        total_loss += loss.item() * valid_tokens
                        
                else:  # è¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼šlabelsæ˜¯[B, N]
                    if logits.dim() == 3:  # [B, N, vocab_size]
                        logits_flat = logits.view(-1, logits.size(-1))
                        labels_flat = labels.view(-1)
                    else:
                        logits_flat = logits
                        labels_flat = labels
                    loss = criterion(logits_flat, labels_flat)
                    
                    # ğŸ¯ è®¡ç®—å›°æƒ‘åº¦ç›¸å…³æŒ‡æ ‡ - åªè®¡ç®—épadding token
                    with torch.no_grad():
                        valid_tokens = (labels_flat != 0).sum().item()
                        if valid_tokens > 0:
                            total_tokens += valid_tokens
                            total_loss += loss.item() * valid_tokens  # åŠ æƒloss
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                _, predicted = logits.max(1)
                correct += predicted.eq(labels).sum().item()
                total += labels.size(0)
                
                global_step += 1
                
                # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
                progress_bar.set_postfix({
                    'Loss': f"{loss.item():.4f}",
                    'Acc': f"{100.*correct/total:.2f}%"
                })
        
        # å…³é—­è¿›åº¦æ¡
        progress_bar.close()
        
        # ğŸ¯ éªŒè¯é˜¶æ®µï¼šè®¡ç®—å›°æƒ‘åº¦
        model.eval()
        val_total_loss = 0.0
        val_total_tokens = 0
        val_num_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                inputs = batch["input_ids"].to(device)
                labels = batch["labels"].to(device)
                attention_mask = batch.get("attention_mask", None)
                if attention_mask is not None:
                    attention_mask = attention_mask.to(device)
                
                outputs = model(inputs, attention_mask=attention_mask, labels=labels)
                logits = outputs["logits"]
                
                # è¯­è¨€å»ºæ¨¡ä»»åŠ¡
                if logits.dim() == 3:  # [B, N, vocab_size]
                    logits_flat = logits.view(-1, logits.size(-1))
                    labels_flat = labels.view(-1)
                else:
                    logits_flat = logits
                    labels_flat = labels
                    
                loss = criterion(logits_flat, labels_flat)
                
                # è®¡ç®—æœ‰æ•ˆtokenæ•°é‡ï¼ˆæ’é™¤paddingï¼‰
                valid_tokens = (labels_flat != 0).sum().item()
                if valid_tokens > 0:
                    val_total_loss += loss.item() * valid_tokens
                    val_total_tokens += valid_tokens
                val_num_batches += 1
        
        # æ”¶é›†æŒ‡é’ˆç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœæ˜¯pointeræ¨¡å‹ï¼‰
        if hasattr(model, 'get_pointer_stats'):
            pointer_stats = model.get_pointer_stats()
            training_metrics['pointer_stats'].append(pointer_stats)
            if epoch == epochs - 1:  # æœ€åä¸€ä¸ªepochæ‰“å°è¯¦ç»†ç»Ÿè®¡
                print(f"Pointer Stats - Utilization: {pointer_stats.get('pointer_utilization', 0):.3f}, "
                      f"Avg Hop Distance: {pointer_stats.get('avg_hop_distance', 0):.2f}, "
                      f"Entropy: {pointer_stats.get('pointer_entropy', 0):.3f}")
        
        # ğŸ¯ è®¡ç®—epochå›°æƒ‘åº¦
        if total_tokens > 0:
            epoch_train_loss = total_loss / total_tokens
            epoch_train_perplexity = torch.exp(torch.tensor(epoch_train_loss)).item()
        else:
            epoch_train_loss = float('inf')
            epoch_train_perplexity = float('inf')
            
        if val_total_tokens > 0:
            epoch_val_loss = val_total_loss / val_total_tokens
            epoch_val_perplexity = torch.exp(torch.tensor(epoch_val_loss)).item()
        else:
            epoch_val_loss = float('inf')
            epoch_val_perplexity = float('inf')
        
        # è®°å½•è®­ç»ƒæŒ‡æ ‡
        training_metrics['train_losses'].append(epoch_train_loss)
        training_metrics['train_perplexities'].append(epoch_train_perplexity)
        training_metrics['val_losses'].append(epoch_val_loss)
        training_metrics['val_perplexities'].append(epoch_val_perplexity)
        
        if max_steps is not None:
            print(f"Completed {global_step} steps: "
                  f"Train Loss: {epoch_train_loss:.4f} | PPL: {epoch_train_perplexity:.2f} | "
                  f"Val Loss: {epoch_val_loss:.4f} | Val PPL: {epoch_val_perplexity:.2f}")
        else:
            print(f"Epoch {epoch+1}: "
                  f"Train Loss: {epoch_train_loss:.4f} | PPL: {epoch_train_perplexity:.2f} | "
                  f"Val Loss: {epoch_val_loss:.4f} | Val PPL: {epoch_val_perplexity:.2f}")
        
        # å¦‚æœè¾¾åˆ°æœ€å¤§æ­¥æ•°ï¼Œæå‰ç»“æŸ
        if max_steps is not None and global_step >= max_steps:
            break
    
    if return_metrics:
        return training_metrics

def run_benchmark(dataset_name, models=["pointer", "transformer"], epochs=10, detailed_analysis=False, max_steps=1000):
    """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
    # åŠ è½½æ•°æ®
    train, val, test, vocab_size, max_len, preprocess = load_dataset(dataset_name)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - ç¡®ä¿max_lenä¼ é€’ç»™preprocess 
    # ä½¿ç”¨set_formatç¡®ä¿è¿”å›pytorch tensors
    train_processed = train.map(lambda x: preprocess(x, max_len=max_len))
    train_processed.set_format(type='torch', columns=['input_ids', 'labels', 'attention_mask'])
    
    val_processed = val.map(lambda x: preprocess(x, max_len=max_len))
    val_processed.set_format(type='torch', columns=['input_ids', 'labels', 'attention_mask'])
    
    test_processed = test.map(lambda x: preprocess(x, max_len=max_len))
    test_processed.set_format(type='torch', columns=['input_ids', 'labels', 'attention_mask'])
    
    train_loader = DataLoader(
        train_processed, 
        batch_size=32,  # å¢åŠ æ‰¹é‡å¤§å°
        shuffle=True
    )
    val_loader = DataLoader(
        val_processed,
        batch_size=32  # å¢åŠ æ‰¹é‡å¤§å°
    )
    test_loader = DataLoader(
        test_processed,
        batch_size=32  # å¢åŠ æ‰¹é‡å¤§å°
    )
    
    results = []
    
    for model_type in models:
        # åˆå§‹åŒ–æ¨¡å‹
        if model_type == "pointer":
            # å¢å¼ºçš„åŒå‘å¤šå¤´æŒ‡é’ˆé…ç½®
            reflection_config = {
                # å…¨å±€åæ€æœºåˆ¶ï¼Œæ— éœ€æŒ‡å®šç‰¹å®šå±‚
                'global_reflection': True,
                'bidirectional_multihead': True,  # å¯ç”¨åŒå‘å¤šå¤´æŒ‡é’ˆ
                'learnable_parameters': True      # ç¡®ä¿æ‰€æœ‰å‚æ•°å¯å­¦ä¹ 
            }
            
            model = PointerDecoder(
                vocab_size=vocab_size,
                d=128,
                n_layers=4,
                n_heads=2,  # å‡å°‘å¤´æ•°ä»4åˆ°2
                max_seq_len=max_len,
                reflection_config=reflection_config  # å¯ç”¨å…¨å±€åæ€å’ŒåŒå‘å¤šå¤´æŒ‡é’ˆ
            )
        elif model_type == "transformer":
            model = TransformerBaseline(vocab_size=vocab_size, d_model=128, n_layers=4)
        else:
            raise ValueError(f"Unknown model type: {model_type}. Supported: ['pointer', 'transformer']")
        
        # è®­ç»ƒå’Œè¯„ä¼°
        print(f"\n=== Benchmarking {model_type} on {dataset_name} ===")
        
        # è®°å½•å†…å­˜å’Œæ—¶é—´
        import time
        import torch.cuda as cuda
        
        start_time = time.time()
        # æ£€æµ‹è®¾å¤‡ç±»å‹å¹¶é‡ç½®å†…å­˜ç»Ÿè®¡
        if torch.backends.mps.is_available():
            device_type = "mps"
            # MPSä¸æ”¯æŒå†…å­˜ç»Ÿè®¡ï¼Œä½¿ç”¨0ä½œä¸ºå ä½
            max_mem = 0
        elif cuda.is_available():
            device_type = "cuda"
            cuda.reset_peak_memory_stats()
        else:
            device_type = "cpu"
            max_mem = 0
        
        train_metrics = train_model(model, train_loader, val_loader, epochs=epochs, return_metrics=detailed_analysis, max_steps=max_steps)
        
        # æ”¶é›†ç»“æœ
        elapsed = time.time() - start_time
        if device_type == "cuda":
            max_mem = cuda.max_memory_allocated() / (1024 ** 2)
        elif device_type == "mps":
            max_mem = 0  # MPSå†…å­˜ç»Ÿè®¡ä¸å¯ç”¨
        else:
            max_mem = 0
        
        # æ”¶é›†è¯¦ç»†ç»“æœ
        model_result = {
            "model": model_type,
            "train_time": elapsed,
            "max_memory": max_mem,
            "device": device_type
        }
        
        # å¦‚æœæœ‰è®­ç»ƒæŒ‡æ ‡ï¼Œæ·»åŠ æœ€ç»ˆæ€§èƒ½
        if detailed_analysis and train_metrics and len(train_metrics.get('train_accs', [])) > 0:
            model_result.update({
                "final_train_acc": train_metrics['train_accs'][-1],
                "final_val_acc": train_metrics['val_accs'][-1],
                "final_train_loss": train_metrics['train_losses'][-1],
                "final_val_loss": train_metrics['val_losses'][-1]
            })
            
            # æŒ‡é’ˆæ¨¡å‹çš„ç‰¹æ®Šç»Ÿè®¡
            if model_type == "pointer" and train_metrics.get('pointer_stats') and len(train_metrics['pointer_stats']) > 0:
                final_pointer_stats = train_metrics['pointer_stats'][-1]
                model_result.update({
                    "pointer_utilization": final_pointer_stats.get('pointer_utilization', 0),
                    "avg_hop_distance": final_pointer_stats.get('avg_hop_distance', 0),
                    "pointer_entropy": final_pointer_stats.get('pointer_entropy', 0)
                })
        else:
            # æ·»åŠ é»˜è®¤å€¼é˜²æ­¢é”™è¯¯
            if detailed_analysis:
                model_result.update({
                    "final_train_acc": 0.0,
                    "final_val_acc": 0.0,
                    "final_train_loss": 0.0,
                    "final_val_loss": 0.0
                })
        
        results.append(model_result)
    
    # æ‰“å°å¢å¼ºçš„ç»¼åˆæŠ¥å‘Š
    print("\n=== Enhanced Benchmark Summary ===")
    print(f"Dataset: {dataset_name}")
    if max_steps is not None:
        print(f"Training: {max_steps} steps (1 epoch)")
    else:
        print(f"Training: {epochs} epochs")
    print(f"Architecture Updates: Bidirectional Multi-Head Pointers, Learnable Parameters")
    
    if detailed_analysis:
        print("\nDetailed Performance Analysis:")
        print("Model\t\tDevice\tTime(s)\tMemory(MB)\tTrain Acc\tVal Acc\tTrain Loss\tVal Loss")
        for r in results:
            print(f"{r['model']:<12}\t{r['device']}\t{r['train_time']:.1f}\t{r['max_memory']:.1f}\t\t"
                  f"{r.get('final_train_acc', 0):.2f}%\t\t{r.get('final_val_acc', 0):.2f}%\t\t"
                  f"{r.get('final_train_loss', 0):.4f}\t\t{r.get('final_val_loss', 0):.4f}")
        
        # æŒ‡é’ˆæ¨¡å‹ç‰¹æ®Šåˆ†æ
        pointer_results = [r for r in results if r['model'] == 'pointer']
        if pointer_results:
            print("\nPointer Architecture Analysis:")
            for r in pointer_results:
                if 'pointer_utilization' in r:
                    print(f"- Pointer Utilization: {r['pointer_utilization']:.3f} (non-self pointing ratio)")
                    print(f"- Average Hop Distance: {r['avg_hop_distance']:.2f} tokens")
                    print(f"- Pointer Entropy: {r['pointer_entropy']:.3f} (relationship diversity)")
    else:
        print("\nBasic Performance Summary:")
        print("Model\t\tDevice\tTrain Time(s)\tMax Memory(MB)")
        for r in results:
            print(f"{r['model']:<12}\t{r['device']}\t{r['train_time']:.2f}\t\t{r['max_memory']:.2f}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", type=str, default="listops-32", 
                        choices=["listops-32", "wikitext-2"])
    parser.add_argument("--model", type=str, nargs="+", default=["pointer", "transformer"],
                        choices=["pointer", "transformer"])
    parser.add_argument("--epochs", type=int, default=10)
    parser.add_argument("--max-steps", type=int, default=1000,
                        help="Maximum training steps (default: 1000)")
    parser.add_argument("--detailed", action="store_true", 
                        help="Enable detailed analysis including pointer statistics")
    args = parser.parse_args()
    
    # å¦‚æœä¼ å…¥å•ä¸ªæ¨¡å‹ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
    if isinstance(args.model, str):
        args.model = [args.model]
    
    run_benchmark(args.dataset, args.model, args.epochs, detailed_analysis=args.detailed, max_steps=args.max_steps)
