import torch
import torch.nn as nn
import math
from typing import Optional, Tuple

try:
    from src.layers.alibi import AliBiPositionalEmbedding, apply_alibi_bias
except ImportError:
    try:
        from layers.alibi import AliBiPositionalEmbedding, apply_alibi_bias
    except ImportError:
        from .alibi import AliBiPositionalEmbedding, apply_alibi_bias


def gather_by_pointer(src, ptr):
    """é€šè¿‡æŒ‡é’ˆæ”¶é›†å€¼ - æ¯ä¸ªä½ç½®åªæœ‰ä¸€ä¸ªæŒ‡é’ˆæŒ‡å‘å¦ä¸€ä¸ªä½ç½®
    
    Args:
        src (torch.Tensor): Source tensor [B, N, d]
        ptr (torch.Tensor): Pointer indices [B, N] - æ¯ä¸ªä½ç½®æŒ‡å‘ä¸€ä¸ªä½ç½®
        
    Returns:
        torch.Tensor: Gathered tensor [B, N, d]
    """
    B, N, d = src.shape
    
    # Clamp indices to valid range
    ptr_clamped = torch.clamp(ptr, 0, N-1)
    
    # Use advanced indexing: each position points to exactly one other position
    batch_idx = torch.arange(B, device=src.device)[:, None]  # [B, 1]
    gathered = src[batch_idx, ptr_clamped]  # [B, N, d]
    
    return gathered


class PointerBlock(nn.Module):
    """
    çº¯å…³ç³»å»ºæ¨¡å— - ä¸“æ³¨å»ºæ¨¡ a-->b çš„æ˜¾å¼å…³ç³»
    
    æ ¸å¿ƒè®¾è®¡ç†å¿µï¼ˆä¼˜åŒ–ç‰ˆï¼‰ï¼š
    1. æ¯ä¸ªtokenç›´æ¥å­¦ä¹ æŒ‡å‘å“ªä¸ªtokenï¼ˆçº¯å…³ç³»å»ºæ¨¡ï¼‰
    2. å…³ç³»é“¾ä¼ é€’ï¼šAâ†’Bâ†’Cï¼Œæ„æˆæ˜¾å¼æ€ç»´é“¾
    3. å»é™¤æ³¨æ„åŠ›æœºåˆ¶ï¼Œä¸“æ³¨å…³ç³»è¡¨ç¤ºï¼šç”¨Nä¸ªå…³ç³»æ›¿ä»£NÃ—Næ³¨æ„åŠ›
    4. å…³ç³»ä½œä¸ºä¸€ç­‰å…¬æ°‘ï¼šç›´æ¥å»ºæ¨¡-->å…³ç³»ï¼Œå¿«é€Ÿæ„å»ºæ€ç»´é“¾
    5. æ”¯æŒåæ€é—¨æ§ï¼šåˆ©ç”¨å†å²å…³ç³»é“¾è¿›è¡Œæ¨ç†
    
    Args:
        d (int): Hidden dimension
        n_heads (int): Number of heads (ç®€åŒ–ä¸ºå…³ç³»å¤´æ•°)
        n_kv_heads (int): Number of key-value heads (for compatibility)
        max_seq_len (int): Maximum sequence length
    """
    
    def __init__(self, d, n_heads, n_kv_heads=None, top_k=1, use_value_proj=True, 
                 use_alibi=False, max_seq_len=4096, addressing_mode='learned'):
        super().__init__()
        self.d = d
        self.n_heads = n_heads
        self.n_kv_heads = n_kv_heads if n_kv_heads is not None else n_heads
        self.head_dim = d // n_heads  
        self.max_seq_len = max_seq_len
        
        assert d % n_heads == 0, f"Hidden dim {d} must be divisible by n_heads {n_heads}"
        
        self.heads_per_kv_group = n_heads // self.n_kv_heads
        
        # ğŸ¯ æ ¸å¿ƒï¼šçº¯å…³ç³»å­¦ä¹ ç½‘ç»œ - ç®€åŒ–è®¾è®¡
        # ç›´æ¥å­¦ä¹  a-->b çš„å…³ç³»æ˜ å°„
        self.relation_encoder = nn.Sequential(
            nn.Linear(d, d // 2),
            nn.GELU(),  # æ›´ç¨³å®šçš„æ¿€æ´»å‡½æ•°
            nn.Linear(d // 2, 1)  # è¾“å‡ºå…³ç³»å¼ºåº¦
        )
        
        # ğŸš€ å…³ç³»å€¼æŠ•å½±ï¼šå°†æºtokenç‰¹å¾è½¬æ¢ä¸ºå…³ç³»ä¼ é€’çš„ä¿¡æ¯
        self.value_proj = nn.Linear(d, d, bias=False) if use_value_proj else nn.Identity()
        
        # ğŸ”¥ å…³ç³»ä¼ é€’ç½‘ç»œï¼šå¤„ç†Aâ†’Bå…³ç³»ä¸­çš„ä¿¡æ¯ä¼ é€’
        self.relation_transform = nn.Sequential(
            nn.Linear(d * 2, d),  # è¾“å…¥ï¼š[source_token, target_token]çš„æ‹¼æ¥
            nn.GELU(),
            nn.Linear(d, d)
        )
        
        # ç®€åŒ–è¾“å‡ºæŠ•å½±
        self.o_proj = nn.Linear(d, d, bias=False)
        
        # å…³é—­AliBiä»¥æå‡é€Ÿåº¦å’Œçº¯å‡€åº¦
        self.use_alibi = False
        
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self):
        """Initialize weights with proper scaling."""
        init_std = 0.02 / math.sqrt(self.d)
        for module in [self.value_proj, self.o_proj]:
            if hasattr(module, 'weight'):
                nn.init.normal_(module.weight, mean=0.0, std=init_std)
    
    def _compute_pure_relations(self, h, prev_idx=None):
        """
        ğŸ¯ çº¯å…³ç³»å»ºæ¨¡ï¼šç›´æ¥å­¦ä¹  a-->b çš„æ˜¾å¼å…³ç³»
        
        Args:
            h (torch.Tensor): Hidden states [B, N, d]
            prev_idx (Optional[torch.Tensor]): Previous layer's relation chain [B, N]
            
        Returns:
            torch.Tensor: Relation targets [B, N] - æ¯ä¸ªtokenæŒ‡å‘çš„ç›®æ ‡token
        """
        B, N, d = h.shape
        device = h.device
        
        # ğŸ¯ æ ¸å¿ƒï¼šç›´æ¥å­¦ä¹ å…³ç³»æ˜ å°„
        # æ¯ä¸ªtokenå­¦ä¹ æŒ‡å‘å“ªä¸ªä½ç½®
        relation_logits = self.relation_encoder(h).squeeze(-1)  # [B, N]
        
        # è½¬æ¢ä¸ºä½ç½®ç´¢å¼•ï¼ˆæ›´ç®€å•ç›´æ¥ï¼‰
        relation_targets = torch.sigmoid(relation_logits) * (N - 1)
        relation_targets = relation_targets.round().long()
        relation_targets = torch.clamp(relation_targets, 0, N - 1)
        
        # ğŸš€ å…³ç³»é“¾ç»§æ‰¿ï¼šåŸºäºprev_idxå½¢æˆæ€ç»´é“¾
        if prev_idx is not None:
            # ç­–ç•¥ï¼šååŠéƒ¨åˆ†tokenæ›´å€¾å‘äºç»§æ‰¿å…³ç³»é“¾
            chain_threshold = N // 2
            should_chain = torch.arange(N, device=device) >= chain_threshold
            should_chain = should_chain.unsqueeze(0).expand(B, N)
            
            # å…³ç³»é“¾ä¼ é€’ï¼šAâ†’B, Bâ†’C => Aâ†’Bâ†’C
            prev_idx_clamped = torch.clamp(prev_idx, 0, N - 1)
            relation_targets = torch.where(should_chain, prev_idx_clamped, relation_targets)
        
        return relation_targets
    
    def _pure_relation_aggregation(self, h, relation_targets):
        """
        ğŸ”¥ çº¯å…³ç³»ä¿¡æ¯èšåˆï¼šå¤„ç† a-->b ä¸­çš„ä¿¡æ¯ä¼ é€’
        
        Args:
            h (torch.Tensor): Source hidden states [B, N, d]
            relation_targets (torch.Tensor): Relation targets [B, N]
            
        Returns:
            torch.Tensor: Relation-aggregated features [B, N, d]
        """
        B, N, d = h.shape
        
        # 1. è·å–å…³ç³»ç›®æ ‡çš„ç‰¹å¾
        batch_idx = torch.arange(B, device=h.device)[:, None]  # [B, 1]
        target_features = h[batch_idx, relation_targets]  # [B, N, d]
        
        # 2. å…³ç³»å€¼æŠ•å½±
        source_values = self.value_proj(h)  # [B, N, d]
        target_values = self.value_proj(target_features)  # [B, N, d]
        
        # 3. ğŸ¯ æ ¸å¿ƒï¼šå…³ç³»ä¼ é€’ç½‘ç»œå¤„ç† sourceâ†’target çš„ä¿¡æ¯æµ
        # æ‹¼æ¥æºå’Œç›®æ ‡ç‰¹å¾ï¼Œå­¦ä¹ å…³ç³»ä¼ é€’
        relation_input = torch.cat([source_values, target_values], dim=-1)  # [B, N, 2d]
        relation_output = self.relation_transform(relation_input)  # [B, N, d]
        
        return relation_output
    
    def forward(self, h, kv_cache=None, prev_idx=None, return_full_scores=False):
        """
        çº¯å…³ç³»å»ºæ¨¡çš„å‰å‘ä¼ æ’­ - ä¸“æ³¨ a-->b æ˜¾å¼å…³ç³»
        
        Args:
            h (torch.Tensor): Input hidden states [B, N, d]
            kv_cache (Optional): KV cache for inference (ç®€åŒ–å¤„ç†)
            prev_idx (Optional[torch.Tensor]): Previous layer relation targets [B, N] for chaining
            return_full_scores (bool): Whether to return full position scores (å…¼å®¹æ€§)
            
        Returns:
            Tuple containing:
                - z (torch.Tensor): Output representations [B, N, d]
                - relation_targets (torch.Tensor): Relation targets [B, N] - each token points to one target
                - relation_strength (torch.Tensor): Relation strength [B, N] - strength of each relation
                - full_scores (Optional): Full scores if requested (for compatibility)
        """
        B, N, d = h.shape
        
        # å¤„ç†ç¼“å­˜ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä¸“æ³¨å…³ç³»å»ºæ¨¡ï¼‰
        if kv_cache is None:
            h_src = h
            N_cache = N
        else:
            # ç®€åŒ–çš„ç¼“å­˜å¤„ç†
            if hasattr(kv_cache, 'get') and kv_cache.get('vals') is not None:
                cached_vals = kv_cache.get('vals')
                cache_pos = kv_cache.get('pos', 0)
                if cache_pos > 0:
                    h_src = cached_vals[:, :cache_pos]
                else:
                    h_src = h
            else:
                h_src = h
            N_cache = h_src.shape[1]
        
        # è¾¹ç•Œæ£€æŸ¥
        if N == 0 or N_cache == 0:
            z = torch.zeros_like(h)
            relation_targets = torch.zeros(B, N, dtype=torch.long, device=h.device)
            relation_strength = torch.zeros(B, N, device=h.device)
            if return_full_scores:
                full_scores = torch.zeros(B, N, N_cache, device=h.device)
                return z, relation_targets, relation_strength, full_scores
            else:
                return z, relation_targets, relation_strength
        
        # ğŸ¯ æ­¥éª¤1ï¼šå­¦ä¹ çº¯å…³ç³» - æ¯ä¸ªtokenå­¦ä¹ æŒ‡å‘å“ªä¸ªtoken
        relation_targets = self._compute_pure_relations(h, prev_idx)  # [B, N]
        
        # ğŸ”¥ æ­¥éª¤2ï¼šå…³ç³»ä¿¡æ¯èšåˆ - å¤„ç† a-->b çš„ä¿¡æ¯ä¼ é€’
        relation_output = self._pure_relation_aggregation(h, relation_targets)  # [B, N, d]
        
        # ğŸš€ æ­¥éª¤3ï¼šè¾“å‡ºæŠ•å½±
        z = self.o_proj(relation_output)
        
        # è®¡ç®—å…³ç³»å¼ºåº¦ï¼ˆç”¨äºå…¼å®¹æ€§å’Œåˆ†æï¼‰
        # ä½¿ç”¨å…³ç³»ç¼–ç å™¨çš„è¾“å‡ºä½œä¸ºå¼ºåº¦æŒ‡æ ‡
        relation_logits = self.relation_encoder(h).squeeze(-1)  # [B, N]
        relation_strength = torch.sigmoid(relation_logits)  # [B, N] å½’ä¸€åŒ–åˆ°[0,1]
        
        if return_full_scores:
            # ä¸ºå…¼å®¹æ€§åˆ›å»ºå…¨åˆ†æ•°çŸ©é˜µï¼ˆå®é™…ä¸Šæ˜¯ç¨€ç–çš„ï¼‰
            full_scores = torch.zeros(B, N, N_cache, device=h.device)
            
            # åœ¨å¯¹åº”çš„å…³ç³»ç›®æ ‡ä½ç½®è®¾ç½®å¼ºåº¦
            batch_idx = torch.arange(B, device=h.device)[:, None]
            seq_idx = torch.arange(N, device=h.device)[None, :]
            relation_targets_clamped = torch.clamp(relation_targets, 0, N_cache - 1)
            full_scores[batch_idx, seq_idx, relation_targets_clamped] = relation_strength
            
            return z, relation_targets, relation_strength, full_scores
        else:
            return z, relation_targets, relation_strength
