import torch
import torch.nn as nn
import math
from typing import Optional, Tuple, List

try:
    from src.layers.alibi import AliBiPositionalEmbedding, apply_alibi_bias
except ImportError:
    try:
        from layers.alibi import AliBiPositionalEmbedding, apply_alibi_bias
    except ImportError:
        from .alibi import AliBiPositionalEmbedding, apply_alibi_bias


def gather_by_pointer(src, ptr):
    """é€šè¿‡æŒ‡é’ˆæ”¶é›†å€¼ - æ”¯æŒå•æŒ‡é’ˆå’Œå¤šè·³æŒ‡é’ˆé“¾
    
    Args:
        src (torch.Tensor): Source tensor [B, N, d]
        ptr (torch.Tensor | List[torch.Tensor]): æŒ‡é’ˆç´¢å¼•æˆ–æŒ‡é’ˆé“¾
                  [B, N] æˆ– List[[B, N], ...]
        
    Returns:
        torch.Tensor | List[torch.Tensor]: æ”¶é›†ç»“æœ
    """
    if isinstance(ptr, list):
        return [gather_by_pointer(src, p) for p in ptr]
    
    # åŸå§‹å•æŒ‡é’ˆé€»è¾‘
    B, N, d = src.shape
    
    # Clamp indices to valid range
    ptr_clamped = torch.clamp(ptr, 0, N-1)
    
    # ç¡®ä¿ptr_clampedå½¢çŠ¶æ­£ç¡® [B, N]
    if ptr_clamped.dim() == 3:
        ptr_clamped = ptr_clamped.squeeze(-1)
    
    # ä½¿ç”¨å¹¿æ’­ç´¢å¼•
    batch_idx = torch.arange(B, device=src.device)[:, None].expand(B, N)  # [B, N]
    gathered = src[batch_idx, ptr_clamped]  # [B, N, d]
    
    return gathered


class PointerChain(nn.Module):
    """å¤šè·³æŒ‡é’ˆé“¾æ¨¡å—"""
    def __init__(self, d, max_hops=3):
        super().__init__()
        self.max_hops = max_hops
        self.hop_norms = nn.ModuleList([nn.LayerNorm(d) for _ in range(max_hops)])
        self.hop_projs = nn.ModuleList([nn.Linear(d, 1) for _ in range(max_hops)])
    
    def forward(self, h, first_hop_ptr):
        """ç”Ÿæˆå¤šè·³æŒ‡é’ˆé“¾
        Args:
            h: [B, N, d] è¾“å…¥ç‰¹å¾
            first_hop_ptr: [B, N] ç¬¬ä¸€è·³æŒ‡é’ˆ
        Returns:
            List[[B, N], ...] å¤šè·³æŒ‡é’ˆé“¾
        """
        ptr_chain = [first_hop_ptr]
        for i in range(1, self.max_hops):
            # è·å–ä¸Šä¸€è·³ç‰¹å¾
            hop_feat = gather_by_pointer(h, ptr_chain[-1])
            # è®¡ç®—ä¸‹ä¸€è·³æŒ‡é’ˆ
            next_ptr = self.hop_projs[i](self.hop_norms[i](hop_feat))
            next_ptr = torch.sigmoid(next_ptr) * (h.size(1) - 1)
            ptr_chain.append(next_ptr.round().long())
        return ptr_chain


class PointerBlock(nn.Module):
    """
    çº¯å…³ç³»å»ºæ¨¡å— - æ”¯æŒå¤šè·³å…³ç³»é“¾ (Aâ†’Bâ†’Câ†’...)
    
    æ ¸å¿ƒè®¾è®¡ç†å¿µï¼ˆä¼˜åŒ–ç‰ˆï¼‰ï¼š
    1. æ¯ä¸ªtokenç›´æ¥å­¦ä¹ æŒ‡å‘å“ªä¸ªtokenï¼ˆçº¯å…³ç³»å»ºæ¨¡ï¼‰
    2. å…³ç³»é“¾ä¼ é€’ï¼šAâ†’Bâ†’Cï¼Œæ„æˆæ˜¾å¼æ€ç»´é“¾
    3. å»é™¤æ³¨æ„åŠ›æœºåˆ¶ï¼Œä¸“æ³¨å…³ç³»è¡¨ç¤ºï¼šç”¨Nä¸ªå…³ç³»æ›¿ä»£NÃ—Næ³¨æ„åŠ›
    4. å…³ç³»ä½œä¸ºä¸€ç­‰å…¬æ°‘ï¼šç›´æ¥å»ºæ¨¡-->å…³ç³»ï¼Œå¿«é€Ÿæ„å»ºæ€ç»´é“¾
    5. æ”¯æŒåæ€é—¨æ§ï¼šåˆ©ç”¨å†å²å…³ç³»é“¾è¿›è¡Œæ¨ç†
    
    Args:
        d (int): Hidden dimension
        n_heads (int): Number of heads (ç®€åŒ–ä¸ºå…³ç³»å¤´æ•°)
        n_kv_heads (int): Number of key-value heads (for compatibility)
        max_seq_len (int): Maximum sequence length
    """
    
    def __init__(self, d, n_heads, n_kv_heads=None, use_value_proj=True,
                 use_alibi=False, max_seq_len=4096, addressing_mode='learned',
                 multi_hop=1, dynamic_threshold=0.3, max_branches=3):  # åŠ¨æ€åˆ†å‰å‚æ•°
        super().__init__()
        self.d = d
        self.n_heads = n_heads
        self.n_kv_heads = n_kv_heads if n_kv_heads is not None else n_heads
        self.head_dim = d // n_heads  
        self.max_seq_len = max_seq_len
        
        assert d % n_heads == 0, f"Hidden dim {d} must be divisible by n_heads {n_heads}"
        
        self.heads_per_kv_group = n_heads // self.n_kv_heads
        
        # ğŸ¯ æ ¸å¿ƒï¼šçº¯å…³ç³»å­¦ä¹ ç½‘ç»œ - ç®€åŒ–è®¾è®¡
        # ç›´æ¥å­¦ä¹  a-->b çš„å…³ç³»æ˜ å°„
        self.relation_encoder = nn.Sequential(
            nn.Linear(d, d // 2),
            nn.GELU(),  # æ›´ç¨³å®šçš„æ¿€æ´»å‡½æ•°
            nn.Linear(d // 2, 1)  # è¾“å‡ºå…³ç³»å¼ºåº¦
        )
        
        # ğŸš€ å…³ç³»å€¼æŠ•å½±ï¼šå°†æºtokenç‰¹å¾è½¬æ¢ä¸ºå…³ç³»ä¼ é€’çš„ä¿¡æ¯
        self.value_proj = nn.Linear(d, d, bias=False) if use_value_proj else nn.Identity()
        
        # ğŸ”¥ å…³ç³»ä¼ é€’ç½‘ç»œï¼šå¤„ç†Aâ†’Bå…³ç³»ä¸­çš„ä¿¡æ¯ä¼ é€’
        self.relation_transform = nn.Sequential(
            nn.Linear(d * 2, d),  # è¾“å…¥ï¼š[source_token, target_token]çš„æ‹¼æ¥
            nn.GELU(),
            nn.Linear(d, d)
        )
        
        # ç®€åŒ–è¾“å‡ºæŠ•å½±
        self.o_proj = nn.Linear(d, d, bias=False)
        
        # å…³é—­AliBiä»¥æå‡é€Ÿåº¦å’Œçº¯å‡€åº¦
        self.use_alibi = False
        
        # å¤šè·³æŒ‡é’ˆæ”¯æŒ
        self.multi_hop = multi_hop
        self.pointer_chain = PointerChain(d, max_hops=multi_hop) if multi_hop > 1 else None
        
        # å¯å­¦ä¹ çš„åŠ¨æ€åˆ†å‰å‚æ•°
        self.dynamic_threshold = nn.Parameter(torch.tensor(dynamic_threshold))
        self.max_branches = max_branches
        # åŠ¨æ€åˆ†å‰å­¦ä¹ ç½‘ç»œ
        self.branch_learner = nn.Sequential(
            nn.Linear(d, d//2),
            nn.GELU(),
            nn.Linear(d//2, 1),
            nn.Sigmoid()
        )
        
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self):
        """Initialize weights with proper scaling."""
        init_std = 0.02 / math.sqrt(self.d)
        for module in [self.value_proj, self.o_proj]:
            if hasattr(module, 'weight'):
                nn.init.normal_(module.weight, mean=0.0, std=init_std)
    
    def _compute_pure_relations(self, h, prev_idx=None):
        """åŠ¨æ€åˆ†å‰å…³ç³»å»ºæ¨¡
        
        Args:
            h (torch.Tensor): Hidden states [B, N, d]
            prev_idx (Optional[torch.Tensor]): Previous layer's relation chain [B, N]
            
        Returns:
            List[torch.Tensor]: åŠ¨æ€ç”Ÿæˆçš„åˆ†å‰æŒ‡é’ˆåˆ—è¡¨ [B, N]
        """
        B, N, d = h.shape
        device = h.device
        
        # è®¡ç®—åŸºç¡€å…³ç³»å¼ºåº¦
        base_logits = self.relation_encoder(h)  # [B, N, 1]
        base_strength = torch.sigmoid(base_logits)  # [B, N, 1]
        
        # åŠ¨æ€åˆ†å‰å†³ç­–
        branch_mask = (base_strength > self.dynamic_threshold).float()  # [B, N, 1]
        num_branches = torch.clamp(
            (base_strength / self.dynamic_threshold).round().long(),
            1, self.max_branches
        )  # [B, N, 1]
        
        # ç”Ÿæˆå¤šåˆ†æ”¯æŒ‡é’ˆ
        all_pointers = []
        for b in range(self.max_branches):
            # æ¯ä¸ªåˆ†æ”¯æœ‰è½»å¾®ä¸åŒçš„å…³ç³»è®¡ç®—
            branch_logits = self.relation_encoder(h + 0.1*b)  # [B, N, 1]
            branch_logits = branch_logits.squeeze(-1)  # [B, N]
            branch_targets = torch.sigmoid(branch_logits) * (N - 1)
            branch_targets = branch_targets.round().long().view(B, N)  # ç¡®ä¿å½¢çŠ¶ä¸º[B, N]
            
            # åªä¿ç•™æœ‰æ•ˆçš„åˆ†æ”¯
            active = (b < num_branches).squeeze(-1)  # [B, N]
            # ç¡®ä¿zeros_likeä¸branch_targetsç»´åº¦ä¸€è‡´
            zeros = torch.zeros_like(branch_targets)
            branch_targets = torch.where(
                active, 
                branch_targets,
                zeros)  # æ— æ•ˆåˆ†æ”¯æŒ‡å‘0
                
            all_pointers.append(branch_targets)
        
        # ä¸»æŒ‡é’ˆæ€»æ˜¯ç¬¬ä¸€ä¸ªåˆ†æ”¯
        main_ptr = all_pointers[0]
        
        # å…³ç³»é“¾ç»§æ‰¿
        if prev_idx is not None:
            chain_threshold = N // 2
            should_chain = torch.arange(N, device=device) >= chain_threshold
            should_chain = should_chain.unsqueeze(0).expand(B, N)
            prev_idx_clamped = torch.clamp(prev_idx, 0, N - 1)
            main_ptr = torch.where(should_chain, prev_idx_clamped, main_ptr)
        
        # æ›´æ–°ç¬¬ä¸€ä¸ªåˆ†æ”¯
        all_pointers[0] = main_ptr
        
        return all_pointers  # List[[B, N], ...]
    
    def _pure_relation_aggregation(self, h, relation_targets):
        """
        ğŸ”¥ çº¯å…³ç³»ä¿¡æ¯èšåˆï¼šæ”¯æŒåŠ¨æ€åˆ†å‰
        
        Args:
            h (torch.Tensor): Source hidden states [B, N, d]
            relation_targets (torch.Tensor | List[torch.Tensor]): 
                 å•è·³[B, N]æˆ–å¤šè·³æŒ‡é’ˆé“¾List[[B, N], ...]
            
        Returns:
            torch.Tensor: Relation-aggregated features [B, N, d]
        """
        # å¤„ç†å¤šè·³æƒ…å†µ
        if isinstance(relation_targets, list) and len(relation_targets) > 0 and isinstance(relation_targets[0], list):
            # å¤šè·³æ¨¡å¼
            all_relation_feats = []
            for ptr_chain in relation_targets:
                chain_feats = []
                for ptr in ptr_chain:
                    target_feat = gather_by_pointer(h, ptr)
                    source_feat = self.value_proj(h)
                    target_feat = self.value_proj(target_feat)
                    chain_feats.append(torch.cat([source_feat, target_feat], dim=-1))
                all_relation_feats.append(torch.mean(torch.stack(chain_feats), dim=0))
            relation_input = torch.mean(torch.stack(all_relation_feats), dim=0)
        elif isinstance(relation_targets, list):  # å•æŒ‡é’ˆå¤šè·³æ¨¡å¼
            # æœ€åä¸€è·³ä½œä¸ºä¸»å…³ç³»
            main_ptr = relation_targets[-1]
            # èšåˆå¤šè·³ä¿¡æ¯
            multi_hop_feats = []
            for ptr in relation_targets:
                target_feat = gather_by_pointer(h, ptr)
                source_feat = self.value_proj(h)
                target_feat = self.value_proj(target_feat)
                relation_feat = torch.cat([source_feat, target_feat], dim=-1)
                multi_hop_feats.append(relation_feat)
            # å¹³å‡å¤šè·³ç‰¹å¾
            relation_input = torch.mean(torch.stack(multi_hop_feats), dim=0)
        else:  # å•è·³æ¨¡å¼
            target_feat = gather_by_pointer(h, relation_targets)
            source_feat = self.value_proj(h)
            target_feat = self.value_proj(target_feat)
            relation_input = torch.cat([source_feat, target_feat], dim=-1)
        
        return self.relation_transform(relation_input)
    
    def forward(self, h, kv_cache=None, prev_idx=None, return_full_scores=False):
        """
        çº¯å…³ç³»å»ºæ¨¡çš„å‰å‘ä¼ æ’­ - ä¸“æ³¨ a-->b æ˜¾å¼å…³ç³»
        
        Args:
            h (torch.Tensor): Input hidden states [B, N, d]
            kv_cache (Optional): KV cache for inference (ç®€åŒ–å¤„ç†)
            prev_idx (Optional[torch.Tensor]): Previous layer relation targets [B, N] for chaining
            return_full_scores (bool): Whether to return full position scores (å…¼å®¹æ€§)
            
        Returns:
            Tuple containing:
                - z (torch.Tensor): Output representations [B, N, d]
                - relation_targets (torch.Tensor): Relation targets [B, N] - each token points to one target
                - relation_strength (torch.Tensor): Relation strength [B, N] - strength of each relation
                - full_scores (Optional): Full scores if requested (for compatibility)
        """
        B, N, d = h.shape
        
        # å¤„ç†ç¼“å­˜ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä¸“æ³¨å…³ç³»å»ºæ¨¡ï¼‰
        if kv_cache is None:
            h_src = h
            N_cache = N
        else:
            # ç®€åŒ–çš„ç¼“å­˜å¤„ç†
            if hasattr(kv_cache, 'get') and kv_cache.get('vals') is not None:
                cached_vals = kv_cache.get('vals')
                cache_pos = kv_cache.get('pos', 0)
                if cache_pos > 0:
                    h_src = cached_vals[:, :cache_pos]
                else:
                    h_src = h
            else:
                h_src = h
            N_cache = h_src.shape[1]
        
        # è¾¹ç•Œæ£€æŸ¥
        if N == 0 or N_cache == 0:
            z = torch.zeros_like(h)
            relation_targets = torch.zeros(B, N, dtype=torch.long, device=h.device)
            relation_strength = torch.zeros(B, N, device=h.device)
            if return_full_scores:
                full_scores = torch.zeros(B, N, N_cache, device=h.device)
                return z, relation_targets, relation_strength, full_scores
            else:
                return z, relation_targets, relation_strength
        
        # ğŸ¯ æ­¥éª¤1ï¼šå­¦ä¹ çº¯å…³ç³»
        first_hop = self._compute_pure_relations(h, prev_idx)  # [B, N] æˆ– List[[B, N],...]
        
        # å¤šè·³æŒ‡é’ˆé“¾ç”Ÿæˆ
        if self.multi_hop > 1 and self.pointer_chain is not None:
            if isinstance(first_hop, list):  # å¤šæŒ‡é’ˆæ¨¡å¼
                relation_targets = [self.pointer_chain(h, ptr) for ptr in first_hop]
            else:  # å•æŒ‡é’ˆæ¨¡å¼
                relation_targets = self.pointer_chain(h, first_hop)
        else:
            relation_targets = first_hop
        
        # ğŸ”¥ æ­¥éª¤2ï¼šå…³ç³»ä¿¡æ¯èšåˆ (è‡ªåŠ¨å¤„ç†å•è·³/å¤šè·³)
        relation_output = self._pure_relation_aggregation(h, relation_targets)  # [B, N, d]
        
        # ç»Ÿä¸€è¿”å›æ ¼å¼
        if isinstance(relation_targets, list) and isinstance(relation_targets[0], list):
            # å¤šæŒ‡é’ˆ+å¤šè·³æ¨¡å¼: è¿”å›ç¬¬ä¸€ä¸ªæŒ‡é’ˆé“¾ä½œä¸ºä¸»æŒ‡é’ˆ
            main_ptr = relation_targets[0][-1]  # å–ç¬¬ä¸€ä¸ªæŒ‡é’ˆé“¾çš„æœ€åä¸€è·³
        elif isinstance(relation_targets, list):
            # å¤šæŒ‡é’ˆå•è·³æ¨¡å¼: è¿”å›ç¬¬ä¸€ä¸ªæŒ‡é’ˆä½œä¸ºä¸»æŒ‡é’ˆ
            main_ptr = relation_targets[0]
        else:
            # å•æŒ‡é’ˆæ¨¡å¼
            main_ptr = relation_targets
        
        # ğŸš€ æ­¥éª¤3ï¼šè¾“å‡ºæŠ•å½±
        z = self.o_proj(relation_output)
        
        # è®¡ç®—å…³ç³»å¼ºåº¦ï¼ˆç”¨äºå…¼å®¹æ€§å’Œåˆ†æï¼‰
        # ä½¿ç”¨å…³ç³»ç¼–ç å™¨çš„è¾“å‡ºä½œä¸ºå¼ºåº¦æŒ‡æ ‡
        relation_logits = self.relation_encoder(h).squeeze(-1)  # [B, N]
        relation_strength = torch.sigmoid(relation_logits)  # [B, N] å½’ä¸€åŒ–åˆ°[0,1]
        
        if return_full_scores:
            # ä¸ºå…¼å®¹æ€§åˆ›å»ºå…¨åˆ†æ•°çŸ©é˜µï¼ˆå®é™…ä¸Šæ˜¯ç¨€ç–çš„ï¼‰
            full_scores = torch.zeros(B, N, N_cache, device=h.device)
            
            # åœ¨å¯¹åº”çš„å…³ç³»ç›®æ ‡ä½ç½®è®¾ç½®å¼ºåº¦
            batch_idx = torch.arange(B, device=h.device)[:, None]
            seq_idx = torch.arange(N, device=h.device)[None, :]
            relation_targets_clamped = torch.clamp(relation_targets, 0, N_cache - 1)
            full_scores[batch_idx, seq_idx, relation_targets_clamped] = relation_strength
            
            return z, main_ptr, relation_strength, full_scores
        else:
            return z, main_ptr, relation_strength
