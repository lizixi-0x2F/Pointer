import torch
import torch.nn as nn
import math
from typing import Optional, Tuple, List

try:
    from src.layers.alibi import AliBiPositionalEmbedding, apply_alibi_bias
except ImportError:
    try:
        from layers.alibi import AliBiPositionalEmbedding, apply_alibi_bias
    except ImportError:
        from .alibi import AliBiPositionalEmbedding, apply_alibi_bias


def gather_by_pointer(src, ptr):
    """é€šè¿‡æŒ‡é’ˆæ”¶é›†å€¼ - æ”¯æŒå•æŒ‡é’ˆå’Œå¤šè·³æŒ‡é’ˆé“¾
    
    Args:
        src (torch.Tensor): Source tensor [B, N, d]
        ptr (torch.Tensor | List[torch.Tensor]): æŒ‡é’ˆç´¢å¼•æˆ–æŒ‡é’ˆé“¾
                  [B, N] æˆ– List[[B, N], ...]
        
    Returns:
        torch.Tensor | List[torch.Tensor]: æ”¶é›†ç»“æœ
    """
    if isinstance(ptr, list):
        return [gather_by_pointer(src, p) for p in ptr]
    
    # åŸå§‹å•æŒ‡é’ˆé€»è¾‘
    B, N, d = src.shape
    
    # Clamp indices to valid range
    ptr_clamped = torch.clamp(ptr, 0, N-1)
    
    # ç¡®ä¿ptr_clampedå½¢çŠ¶æ­£ç¡® [B, N]
    if ptr_clamped.dim() == 3:
        ptr_clamped = ptr_clamped.squeeze(-1)
    
    # ä½¿ç”¨å¹¿æ’­ç´¢å¼•
    batch_idx = torch.arange(B, device=src.device)[:, None].expand(B, N)  # [B, N]
    gathered = src[batch_idx, ptr_clamped]  # [B, N, d]
    
    return gathered


class PointerChain(nn.Module):
    """å¤šè·³æŒ‡é’ˆé“¾æ¨¡å—"""
    def __init__(self, d, max_hops=3):
        super().__init__()
        self.max_hops = max_hops
        self.hop_norms = nn.ModuleList([nn.LayerNorm(d) for _ in range(max_hops)])
        self.hop_projs = nn.ModuleList([nn.Linear(d, 1) for _ in range(max_hops)])
    
    def forward(self, h, first_hop_ptr):
        """ç”Ÿæˆå¤šè·³æŒ‡é’ˆé“¾
        Args:
            h: [B, N, d] è¾“å…¥ç‰¹å¾
            first_hop_ptr: [B, N] ç¬¬ä¸€è·³æŒ‡é’ˆ
        Returns:
            List[[B, N], ...] å¤šè·³æŒ‡é’ˆé“¾
        """
        ptr_chain = [first_hop_ptr]
        for i in range(1, self.max_hops):
            # è·å–ä¸Šä¸€è·³ç‰¹å¾
            hop_feat = gather_by_pointer(h, ptr_chain[-1])
            # è®¡ç®—ä¸‹ä¸€è·³æŒ‡é’ˆ
            next_ptr = self.hop_projs[i](self.hop_norms[i](hop_feat))
            next_ptr = torch.sigmoid(next_ptr) * (h.size(1) - 1)
            ptr_chain.append(next_ptr.round().long())
        return ptr_chain


class BiDirectionalMultiHeadPointer(nn.Module):
    """
    åŒå‘å¤šå¤´æŒ‡é’ˆæœºåˆ¶ - æ”¯æŒä¸åŒå°ºåº¦çš„å…³ç³»å»ºæ¨¡
    
    æ ¸å¿ƒè®¾è®¡ç†å¿µï¼š
    1. åŒå‘æŒ‡é’ˆï¼šå‰å‘å’Œåå‘å…³ç³»å»ºæ¨¡
    2. å¤šå¤´æœºåˆ¶ï¼šä¸åŒå¤´å…³æ³¨ä¸åŒå°ºåº¦çš„å…³ç³»
    3. å…³ç³»èåˆï¼šæ•´åˆå¤šä¸ªæ–¹å‘å’Œå°ºåº¦çš„ä¿¡æ¯
    
    Args:
        d (int): Hidden dimension
        n_heads (int): Number of relation heads
        max_seq_len (int): Maximum sequence length
    """
    
    def __init__(self, d, n_heads, max_seq_len=4096):
        super().__init__()
        self.d = d
        self.n_heads = n_heads
        self.head_dim = d // n_heads
        self.max_seq_len = max_seq_len
        
        assert d % n_heads == 0, f"Hidden dim {d} must be divisible by n_heads {n_heads}"
        
        # å‰å‘å’Œåå‘å…³ç³»ç¼–ç å™¨
        self.forward_encoders = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d, d // 2),
                nn.GELU(),
                nn.Linear(d // 2, 1)
            ) for _ in range(n_heads)
        ])
        
        self.backward_encoders = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d, d // 2),
                nn.GELU(),
                nn.Linear(d // 2, 1)
            ) for _ in range(n_heads)
        ])
        
        # å¤šå¤´å€¼æŠ•å½±
        self.multi_head_value_proj = nn.ModuleList([
            nn.Linear(d, self.head_dim, bias=False) for _ in range(n_heads)
        ])
        
        # åŒå‘å…³ç³»èåˆç½‘ç»œ
        self.relation_fusion = nn.ModuleList([
            nn.Sequential(
                nn.Linear(self.head_dim * 3, self.head_dim),  # [source, forward_target, backward_target]
                nn.GELU(),
                nn.Linear(self.head_dim, self.head_dim)
            ) for _ in range(n_heads)
        ])
        
        # å¤šå¤´è¾“å‡ºèåˆ
        self.output_proj = nn.Linear(d, d, bias=False)
        
        # å¯å­¦ä¹ çš„é“¾å¼ä¼ æ‰¿å‚æ•°
        self.chain_threshold_ratio = nn.Parameter(torch.tensor(0.5))  # å¯å­¦ä¹ çš„é“¾é˜ˆå€¼æ¯”ä¾‹
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        init_std = 0.02 / math.sqrt(self.d)
        for module_list in [self.multi_head_value_proj]:
            for module in module_list:
                if hasattr(module, 'weight'):
                    nn.init.normal_(module.weight, mean=0.0, std=init_std)
        
        if hasattr(self.output_proj, 'weight'):
            nn.init.normal_(self.output_proj.weight, mean=0.0, std=init_std)
    
    def forward(self, h, prev_idx=None):
        """
        åŒå‘å¤šå¤´æŒ‡é’ˆå‰å‘ä¼ æ’­
        
        Args:
            h (torch.Tensor): Input hidden states [B, N, d]
            prev_idx (Optional[torch.Tensor]): Previous layer pointers [B, N]
            
        Returns:
            Tuple containing:
                - output (torch.Tensor): Output features [B, N, d]
                - forward_pointers (torch.Tensor): Forward pointers [B, N]
                - backward_pointers (torch.Tensor): Backward pointers [B, N]
                - relation_strength (torch.Tensor): Combined relation strength [B, N]
        """
        B, N, d = h.shape
        device = h.device
        
        all_head_outputs = []
        all_forward_pointers = []
        all_backward_pointers = []
        all_strengths = []
        
        for head_idx in range(self.n_heads):
            # è®¡ç®—å‰å‘å’Œåå‘å…³ç³»
            forward_logits = self.forward_encoders[head_idx](h).squeeze(-1)  # [B, N]
            backward_logits = self.backward_encoders[head_idx](h).squeeze(-1)  # [B, N]
            
            # è½¬æ¢ä¸ºå¯å¾®åˆ†çš„æŒ‡é’ˆä½ç½® (ä½¿ç”¨softmaxåˆ†å¸ƒè€Œä¸æ˜¯ç¡¬ä½ç½®)
            forward_probs = torch.softmax(forward_logits.unsqueeze(-1).expand(-1, -1, N), dim=-1)  # [B, N, N]
            backward_probs = torch.softmax(backward_logits.unsqueeze(-1).expand(-1, -1, N), dim=-1)  # [B, N, N]
            
            # è®¡ç®—æœŸæœ›ä½ç½®ç”¨äºç»Ÿè®¡ï¼ˆä¸å‚ä¸æ¢¯åº¦ï¼‰
            position_range = torch.arange(N, device=device, dtype=torch.float).unsqueeze(0).unsqueeze(0)  # [1, 1, N]
            forward_targets = torch.sum(forward_probs * position_range, dim=-1).long()  # [B, N] 
            backward_targets = torch.sum(backward_probs * position_range, dim=-1).long()  # [B, N]
            
            # é“¾å¼ä¼ æ‰¿ï¼ˆå¦‚æœæœ‰å‰ä¸€å±‚çš„æŒ‡é’ˆï¼‰- ä½¿ç”¨å¯å­¦ä¹ çš„é˜ˆå€¼
            if prev_idx is not None:
                chain_threshold = int(torch.sigmoid(self.chain_threshold_ratio) * N)  # å¯å­¦ä¹ çš„é˜ˆå€¼
                should_chain = torch.arange(N, device=device) >= chain_threshold
                should_chain = should_chain.unsqueeze(0).expand(B, N)
                prev_idx_clamped = torch.clamp(prev_idx, 0, N - 1)
                forward_targets = torch.where(should_chain, prev_idx_clamped, forward_targets)
            
            # æå–å¤šå¤´ç‰¹å¾
            head_features = self.multi_head_value_proj[head_idx](h)  # [B, N, head_dim]
            
            # å¯å¾®åˆ†çš„åŒå‘ç‰¹å¾æ”¶é›† (ä½¿ç”¨æ¦‚ç‡åŠ æƒ)
            forward_features = torch.bmm(forward_probs, head_features)  # [B, N, head_dim]
            backward_features = torch.bmm(backward_probs, head_features)  # [B, N, head_dim]
            
            # ä¸‰å…ƒå…³ç³»èåˆï¼š[source, forward_target, backward_target]
            relation_input = torch.cat([head_features, forward_features, backward_features], dim=-1)
            head_output = self.relation_fusion[head_idx](relation_input)  # [B, N, head_dim]
            
            # è®¡ç®—å…³ç³»å¼ºåº¦ (ä½¿ç”¨æ¦‚ç‡åˆ†å¸ƒçš„é›†ä¸­åº¦)
            forward_strength = 1.0 - torch.sum(forward_probs * torch.log(forward_probs + 1e-8), dim=-1)  # ç†µçš„è´Ÿå€¼
            backward_strength = 1.0 - torch.sum(backward_probs * torch.log(backward_probs + 1e-8), dim=-1)
            combined_strength = (forward_strength + backward_strength) / 2
            
            all_head_outputs.append(head_output)
            all_forward_pointers.append(forward_targets)
            all_backward_pointers.append(backward_targets)
            all_strengths.append(combined_strength)
        
        # å¤šå¤´è¾“å‡ºèåˆ
        multi_head_output = torch.cat(all_head_outputs, dim=-1)  # [B, N, d]
        final_output = self.output_proj(multi_head_output)
        
        # èšåˆæŒ‡é’ˆï¼ˆå–ç¬¬ä¸€ä¸ªå¤´çš„æŒ‡é’ˆä½œä¸ºä¸»æŒ‡é’ˆï¼‰
        main_forward_ptr = all_forward_pointers[0]
        main_backward_ptr = all_backward_pointers[0]
        avg_strength = torch.stack(all_strengths, dim=0).mean(dim=0)
        
        return final_output, main_forward_ptr, main_backward_ptr, avg_strength

class PointerBlock(nn.Module):
    """
    é‡æ„çš„æŒ‡é’ˆå— - åŸºäºåŒå‘å¤šå¤´æŒ‡é’ˆæœºåˆ¶
    
    æ ¸å¿ƒè®¾è®¡ç†å¿µï¼š
    1. åŒå‘å…³ç³»å»ºæ¨¡ï¼šå‰å‘å’Œåå‘æŒ‡é’ˆ
    2. å¤šå¤´æœºåˆ¶ï¼šä¸åŒå¤´å…³æ³¨ä¸åŒå°ºåº¦çš„å…³ç³»
    3. å¤šè·³æ”¯æŒï¼šå¯é€‰çš„æŒ‡é’ˆé“¾ä¼ é€’
    4. çº¯å…³ç³»ä¸“æ³¨ï¼šå»é™¤æ³¨æ„åŠ›å¤æ‚æ€§ï¼Œä¸“æ³¨å…³ç³»è¡¨ç¤º
    
    Args:
        d (int): Hidden dimension
        n_heads (int): Number of heads
        n_kv_heads (int): Number of key-value heads (for compatibility)
        max_seq_len (int): Maximum sequence length
        multi_hop (int): Number of hops for pointer chains
    """
    
    def __init__(self, d, n_heads, n_kv_heads=None, use_value_proj=True,
                 use_alibi=False, max_seq_len=4096, addressing_mode='learned',
                 multi_hop=1):
        super().__init__()
        self.d = d
        self.n_heads = n_heads
        self.n_kv_heads = n_kv_heads if n_kv_heads is not None else n_heads
        self.head_dim = d // n_heads  
        self.max_seq_len = max_seq_len
        
        assert d % n_heads == 0, f"Hidden dim {d} must be divisible by n_heads {n_heads}"
        
        # æ ¸å¿ƒï¼šåŒå‘å¤šå¤´æŒ‡é’ˆæœºåˆ¶
        self.bidirectional_pointer = BiDirectionalMultiHeadPointer(d, n_heads, max_seq_len)
        
        # å¤šè·³æŒ‡é’ˆæ”¯æŒ
        self.multi_hop = multi_hop
        self.pointer_chain = PointerChain(d, max_hops=multi_hop) if multi_hop > 1 else None
        
        # å…¼å®¹æ€§ï¼šä¿ç•™åŸæœ‰çš„è¾“å‡ºæŠ•å½±
        self.o_proj = nn.Linear(d, d, bias=False)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """Initialize weights with proper scaling."""
        init_std = 0.02 / math.sqrt(self.d)
        if hasattr(self.o_proj, 'weight'):
            nn.init.normal_(self.o_proj.weight, mean=0.0, std=init_std)
    
    def forward(self, h, kv_cache=None, prev_idx=None, return_full_scores=False):
        """
        åŒå‘å¤šå¤´æŒ‡é’ˆå‰å‘ä¼ æ’­
        
        Args:
            h (torch.Tensor): Input hidden states [B, N, d]
            kv_cache (Optional): KV cache for inference (ç®€åŒ–å¤„ç†)
            prev_idx (Optional[torch.Tensor]): Previous layer relation targets [B, N] for chaining
            return_full_scores (bool): Whether to return full position scores (å…¼å®¹æ€§)
            
        Returns:
            Tuple containing:
                - z (torch.Tensor): Output representations [B, N, d]
                - main_pointer (torch.Tensor): Main pointer targets [B, N]
                - relation_strength (torch.Tensor): Relation strength [B, N]
                - full_scores (Optional): Full scores if requested (for compatibility)
        """
        B, N, d = h.shape
        
        # å¤„ç†ç¼“å­˜ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        if kv_cache is None:
            h_src = h
            N_cache = N
        else:
            # ç®€åŒ–çš„ç¼“å­˜å¤„ç†
            if hasattr(kv_cache, 'get') and kv_cache.get('vals') is not None:
                cached_vals = kv_cache.get('vals')
                cache_pos = kv_cache.get('pos', 0)
                if cache_pos > 0:
                    h_src = cached_vals[:, :cache_pos]
                else:
                    h_src = h
            else:
                h_src = h
            N_cache = h_src.shape[1]
        
        # è¾¹ç•Œæ£€æŸ¥
        if N == 0 or N_cache == 0:
            z = torch.zeros_like(h)
            main_pointer = torch.zeros(B, N, dtype=torch.long, device=h.device)
            relation_strength = torch.zeros(B, N, device=h.device)
            if return_full_scores:
                full_scores = torch.zeros(B, N, N_cache, device=h.device)
                return z, main_pointer, relation_strength, full_scores
            else:
                return z, main_pointer, relation_strength
        
        # ğŸ¯ æ ¸å¿ƒï¼šåŒå‘å¤šå¤´æŒ‡é’ˆè®¡ç®—
        pointer_output, forward_ptr, backward_ptr, relation_strength = self.bidirectional_pointer(h, prev_idx)
        
        # å¤šè·³æŒ‡é’ˆé“¾ç”Ÿæˆï¼ˆå¯é€‰ï¼‰
        if self.multi_hop > 1 and self.pointer_chain is not None:
            forward_chain = self.pointer_chain(h, forward_ptr)
            main_pointer = forward_chain[-1]  # æœ€åä¸€è·³ä½œä¸ºä¸»æŒ‡é’ˆ
        else:
            main_pointer = forward_ptr  # å‰å‘æŒ‡é’ˆä½œä¸ºä¸»æŒ‡é’ˆ
        
        # ğŸš€ è¾“å‡ºæŠ•å½±
        z = self.o_proj(pointer_output)
        
        if return_full_scores:
            # ä¸ºå…¼å®¹æ€§åˆ›å»ºå…¨åˆ†æ•°çŸ©é˜µ
            full_scores = torch.zeros(B, N, N_cache, device=h.device)
            
            # åœ¨å¯¹åº”çš„å…³ç³»ç›®æ ‡ä½ç½®è®¾ç½®å¼ºåº¦
            batch_idx = torch.arange(B, device=h.device)[:, None]
            seq_idx = torch.arange(N, device=h.device)[None, :]
            main_ptr_clamped = torch.clamp(main_pointer, 0, N_cache - 1)
            full_scores[batch_idx, seq_idx, main_ptr_clamped] = relation_strength
            
            return z, main_pointer, relation_strength, full_scores
        else:
            return z, main_pointer, relation_strength
