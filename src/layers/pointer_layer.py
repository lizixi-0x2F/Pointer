import torch
import torch.nn as nn
from typing import Optional, Tuple

try:
    from src.layers.pointer_block import PointerBlock
    from src.layers.llama_mlp import LlamaMLP
    from src.layers.rmsnorm import RMSNorm
except ImportError:
    # Alternative import paths
    try:
        from layers.pointer_block import PointerBlock
        from layers.llama_mlp import LlamaMLP
        from layers.rmsnorm import RMSNorm
    except ImportError:
        # Relative imports
        from .pointer_block import PointerBlock
        from .llama_mlp import LlamaMLP
        from .rmsnorm import RMSNorm


class PointerLayer(nn.Module):
    """DeepSeek-style Pointer Layer with Reflection Mechanism.
    
    Architecture: RMSNorm â†’ PointerBlock â†’ Residual â†’ RMSNorm â†’ SwiGLU â†’ Residual (Pre-norm)
    Key features: 
    - Passes prev_index to form pointer chains across layers
    - Reflection gating mechanism for structured reasoning
    - Pointer backtracking for reflection layers
    
    Args:
        d (int): Hidden dimension
        n_heads (int): Number of attention heads
        layer_idx (int): Current layer index (for reflection control)
        top_k (int): Number of top positions for pointer selection
        d_ff (int): Feed-forward hidden dimension (if None, auto-calculated)
        dropout (float): Dropout rate
        use_value_proj (bool): Whether to use value projection in PointerBlock
        use_alibi (bool): Whether to use AliBi positional encoding
        max_seq_len (int): Maximum sequence length
        reflection_config (dict): Reflection configuration parameters
    """
    
    def __init__(self, d, n_heads, layer_idx=0, n_kv_heads=None, d_ff=None, dropout=0.0,
                 use_value_proj=True, use_alibi=True, max_seq_len=4096, reflection_config=None,
                 dynamic_threshold=None, max_branches=None):
        super().__init__()
        self.d = d
        self.layer_idx = layer_idx
        
        # ğŸ”¥ å¯å­¦ä¹ çš„åˆ†å‰å‚æ•°ï¼šå®Œå…¨ç”±ç½‘ç»œå­¦ä¹ 
        self.learnable_branch_gate = nn.Linear(d, 1)  # å­¦ä¹ æ˜¯å¦åˆ†å‰
        self.learnable_branch_count = nn.Linear(d, 4)  # å­¦ä¹ åˆ†å‰æ•°é‡(1-4)
        self.branch_threshold = nn.Parameter(torch.tensor(0.0))  # å¯å­¦ä¹ çš„é˜ˆå€¼
        
        # ğŸ§  å…¨å±€åæ€æœºåˆ¶ï¼šæ¯ä¸ªå±‚éƒ½é»˜è®¤å…·å¤‡
        self.reflection_config = reflection_config or {}
        # ç§»é™¤ç‰¹å®šå±‚é…ç½®ï¼Œæ‰€æœ‰å±‚éƒ½æœ‰åæ€èƒ½åŠ›
        self.global_reflection_gate = nn.Linear(d, 1)  # å­¦ä¹ ä½•æ—¶å¯ç”¨åæ€
        self.reflection_intensity = nn.Parameter(torch.tensor(0.1))  # å¯å­¦ä¹ çš„åæ€å¼ºåº¦
        self.reflection_norm = RMSNorm(d)
        self.reflection_proj = nn.Linear(d, d, bias=False)
        # ç§»é™¤backtrack_layersé™åˆ¶ï¼Œæ”¹ä¸ºä½¿ç”¨å…¨éƒ¨å†å²å±‚
        
        # DeepSeek-style RMSNorm (Pre-norm architecture)
        self.norm1 = RMSNorm(d)
        self.norm2 = RMSNorm(d)
        
        # Pointer block (ä½¿ç”¨å¯å­¦ä¹ çš„åˆ†å‰å‚æ•°)
        self.pointer_block = PointerBlock(
            d=d,
            n_heads=n_heads,
            n_kv_heads=n_kv_heads,
            use_value_proj=use_value_proj,
            use_alibi=use_alibi,
            max_seq_len=max_seq_len,
            # ä¸å†ä¼ å…¥å›ºå®šçš„thresholdå’Œbranchesï¼Œç”±layeråŠ¨æ€å†³å®š
        )
        
        # Learnable gate for pointer output (preserves original design)
        self.gate = nn.Parameter(torch.ones(d))
        
        # ç§»é™¤ç‰¹å®šå±‚åæ€é…ç½® - ç°åœ¨æ‰€æœ‰å±‚éƒ½æœ‰åæ€èƒ½åŠ›
        # if self.is_reflection_layer: è¿™ä¸ªæ¡ä»¶åˆ¤æ–­å·²ç»ä¸éœ€è¦äº†
        
        # Llama-style SwiGLU FFN
        self.ffn = LlamaMLP(
            hidden_size=d,
            intermediate_size=d_ff or int(8 * d / 3),
            hidden_act="silu"
        )
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
        print(f"PointerLayer {layer_idx} initialized (Learnable): d={d}, n_heads={n_heads}, "
              f"learnable_branching=True, global_reflection=True")
    
    def _apply_global_reflection_mechanism(self, h, layer_history=None, pointer_history=None):
        """Apply learnable global reflection mechanism.
        
        å…¨å±€åæ€æœºåˆ¶ - æ¯ä¸ªå±‚éƒ½å…·å¤‡åæ€èƒ½åŠ›ï¼Œé€šè¿‡å¯å­¦ä¹ å‚æ•°å†³å®šï¼š
        1. æ˜¯å¦å¯ç”¨åæ€ (learnable gate)
        2. åæ€å¼ºåº¦ (learnable intensity)
        3. å…¨å±€å†å²çŠ¶æ€çš„èåˆ
        
        Args:
            h (torch.Tensor): Current hidden states [B, N, d]
            layer_history (List[torch.Tensor]): History of hidden states from all previous layers
            pointer_history (List[torch.Tensor]): History of pointer indices from all previous layers
            
        Returns:
            torch.Tensor: Reflection-enhanced hidden states [B, N, d]
        """
        if layer_history is None or len(layer_history) == 0:
            return h
        
        B, N, d = h.shape
        
        # ğŸ§  å¯å­¦ä¹ çš„åæ€é—¨æ§ï¼šå†³å®šæ˜¯å¦å¯ç”¨åæ€
        reflection_gate = torch.sigmoid(self.global_reflection_gate(h))  # [B, N, 1]
        
        # ğŸŒ å…¨å±€å†å²çŠ¶æ€èšåˆ
        global_context = self._compute_global_context(h, layer_history, pointer_history)
        
        # ğŸ”¥ å¯å­¦ä¹ çš„åæ€ç‰¹å¾ç”Ÿæˆ
        reflection_features = self.reflection_proj(self.reflection_norm(global_context))
        
        # ğŸ¯ åŠ¨æ€åæ€å¼ºåº¦è°ƒåˆ¶
        dynamic_intensity = torch.sigmoid(self.reflection_intensity) * reflection_gate
        
        # Apply reflection
        reflected_h = h + dynamic_intensity * reflection_features
        
        # ä¿å­˜åæ€ç‰¹å¾ç”¨äºåˆ†æ
        self.last_reflection_features = reflection_features.clone()
        self.last_reflection_gate = reflection_gate.clone()
        
        return reflected_h
    
    def _compute_global_context(self, h, layer_history, pointer_history):
        """è®¡ç®—å…¨å±€ä¸Šä¸‹æ–‡ - èåˆæ‰€æœ‰å†å²å±‚çš„ä¿¡æ¯
        
        Args:
            h: å½“å‰éšçŠ¶æ€ [B, N, d]
            layer_history: å†å²å±‚çŠ¶æ€åˆ—è¡¨
            pointer_history: å†å²æŒ‡é’ˆåˆ—è¡¨
            
        Returns:
            global_context: å…¨å±€ä¸Šä¸‹æ–‡ [B, N, d]
        """
        B, N, d = h.shape
        
        if not layer_history:
            return h
        
        # ç®€å•è€Œæœ‰æ•ˆçš„å…¨å±€èšåˆï¼šåŠ æƒå¹³å‡å†å²çŠ¶æ€
        # æƒé‡éšå†å²å±‚çš„è·ç¦»é€’å‡
        weighted_states = []
        total_weight = 0
        
        for i, hist_state in enumerate(layer_history):
            # è·ç¦»æƒé‡ï¼šæœ€è¿‘çš„å±‚æƒé‡æ›´é«˜
            weight = 0.8 ** i  # æŒ‡æ•°è¡°å‡
            weighted_states.append(weight * hist_state)
            total_weight += weight
        
        if total_weight > 0:
            global_history = torch.stack(weighted_states, dim=0).sum(dim=0) / total_weight
        else:
            global_history = layer_history[-1]  # fallback
        
        # èåˆå½“å‰çŠ¶æ€å’Œå…¨å±€å†å²
        alpha = 0.7  # å½“å‰çŠ¶æ€æƒé‡
        beta = 0.3   # å†å²çŠ¶æ€æƒé‡
        global_context = alpha * h + beta * global_history
        
        return global_context
    
    def forward(self, h, kv_cache=None, prev_idx=None, layer_history=None, pointer_history=None, return_full_scores=False):
        """DeepSeek-style forward pass with learnable branching and global reflection.
        
        Args:
            h (torch.Tensor): Input hidden states [B, N, d]
            kv_cache (Optional): KV cache for inference
            prev_idx (Optional[torch.Tensor]): Previous layer's pointer indices for chaining
            layer_history (Optional[List[torch.Tensor]]): History of hidden states for global reflection
            pointer_history (Optional[List[torch.Tensor]]): History of pointer indices for global reflection
            return_full_scores (bool): Whether to return full position scores
            
        Returns:
            Tuple containing:
                - h (torch.Tensor): Output hidden states [B, N, d]
                - idx (torch.Tensor): Current layer's pointer indices [B, N] 
                - p (torch.Tensor): Pointer probabilities [B, N]
                - full_scores (Optional[torch.Tensor]): Full position scores if requested
        """
        # ğŸ§  å…¨å±€åæ€æœºåˆ¶ï¼šæ¯ä¸ªå±‚éƒ½é»˜è®¤å…·å¤‡ï¼Œé€šè¿‡å¯å­¦ä¹ å‚æ•°æ§åˆ¶
        h = self._apply_global_reflection_mechanism(h, layer_history, pointer_history)
        
        # --- Pointer part (Pre-norm) ---
        residual = h
        
        # Pre-norm: normalize then compute
        h_norm = self.norm1(h)
        
        # ğŸ”¥ å¯å­¦ä¹ åˆ†å‰å†³ç­–
        branch_gate_logits = self.learnable_branch_gate(h_norm)  # [B, N, 1]
        branch_count_logits = self.learnable_branch_count(h_norm)  # [B, N, 4]
        
        # å†³å®šæ˜¯å¦åˆ†å‰å’Œåˆ†å‰æ•°é‡
        should_branch = torch.sigmoid(branch_gate_logits + self.branch_threshold) > 0.5  # [B, N, 1]
        branch_count = torch.softmax(branch_count_logits, dim=-1).argmax(dim=-1) + 1  # [B, N] range 1-4
        
        # åŠ¨æ€è°ƒæ•´PointerBlockçš„è¡Œä¸ºï¼ˆè¿™é‡Œéœ€è¦PointerBlockæ”¯æŒåŠ¨æ€å‚æ•°ï¼‰
        # æš‚æ—¶ä½¿ç”¨æ ‡å‡†çš„pointer blockï¼Œåç»­å¯ä»¥æ‰©å±•
        pointer_result = self.pointer_block(h_norm, kv_cache, prev_idx=prev_idx, return_full_scores=return_full_scores)
        
        # Always ensure we have the right number of values
        if return_full_scores:
            if len(pointer_result) == 4:
                z, idx, p, full_scores = pointer_result
            else:
                z, idx, p = pointer_result
                full_scores = None  # Fallback if PointerBlock doesn't return full_scores
        else:
            if len(pointer_result) == 4:
                z, idx, p, full_scores = pointer_result
                full_scores = None  # We don't need it
            else:
                z, idx, p = pointer_result
            full_scores = None
        
        # Apply gate and residual connection
        h = residual + self.gate * self.dropout(z)
        
        # --- SwiGLU FFN part (Pre-norm) ---
        residual = h
        
        # Pre-norm: normalize then compute
        h_norm = self.norm2(h)
        
        # Apply SwiGLU FFN
        ffn_out = self.ffn(h_norm)
        
        # Residual connection
        h = residual + self.dropout(ffn_out)
        
        if return_full_scores:
            return h, idx, p, full_scores
        else:
            return h, idx, p
