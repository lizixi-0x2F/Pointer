import torch
import torch.nn as nn
import math
from typing import Optional, Tuple, List

try:
    from src.layers.pointer_block import PointerBlock
    from src.layers.llama_mlp import LlamaMLP
    from src.layers.rmsnorm import RMSNorm
except ImportError:
    # Alternative import paths
    try:
        from layers.pointer_block import PointerBlock
        from layers.llama_mlp import LlamaMLP
        from layers.rmsnorm import RMSNorm
    except ImportError:
        # Relative imports
        from .pointer_block import PointerBlock
        from .llama_mlp import LlamaMLP
        from .rmsnorm import RMSNorm


class PointerChainReflection(nn.Module):
    """
    åŸºäºæŒ‡é’ˆé“¾çš„åæ€æœºåˆ¶ - è½»é‡çº§ã€å¯è§£é‡Šçš„æŒ‡é’ˆå†å²åˆ†æ
    
    æ ¸å¿ƒç†å¿µï¼š
    1. åªå­˜å‚¨æŒ‡é’ˆç´¢å¼•ï¼Œå†…å­˜å¼€é”€ä» O(L*N*d) é™åˆ° O(L*N)
    2. åˆ†ææŒ‡é’ˆé“¾æ¨¡å¼ï¼šè‡ªå¾ªç¯ã€é•¿è·³ã€æ”¶æ•›ç­‰
    3. åŸºäºæŒ‡é’ˆå…³ç³»å†å²ç”Ÿæˆå½“å‰å±‚çš„æŒ‡é’ˆè°ƒæ•´å»ºè®®
    4. å®Œå…¨å¯è§£é‡Šï¼šæ¯ä¸ªåæ€å†³ç­–éƒ½å¯è¿½æº¯åˆ°å…·ä½“æŒ‡é’ˆè·¯å¾„
    
    Args:
        d (int): Hidden dimension (ç”¨äºç‰¹å¾ç¼–ç )
        max_history_layers (int): æœ€å¤§å†å²å±‚æ•°
        pattern_analysis_dim (int): æŒ‡é’ˆæ¨¡å¼åˆ†æçš„ç‰¹å¾ç»´åº¦
    """
    
    def __init__(self, d, max_history_layers=8, pattern_analysis_dim=64):
        super().__init__()
        self.d = d
        self.max_history_layers = max_history_layers
        self.pattern_dim = pattern_analysis_dim
        
        # ğŸ” æŒ‡é’ˆæ¨¡å¼è¯†åˆ«ç½‘ç»œ
        self.pattern_analyzer = nn.Sequential(
            nn.Linear(max_history_layers, pattern_analysis_dim),
            nn.GELU(),
            nn.Linear(pattern_analysis_dim, pattern_analysis_dim // 2),
            nn.GELU(),
            nn.Linear(pattern_analysis_dim // 2, 4)  # 4ç§åŸºæœ¬æ¨¡å¼ï¼šè‡ªå¾ªç¯ã€çŸ­è·³ã€é•¿è·³ã€éšæœº
        )
        
        # ğŸ¯ åŸºäºæ¨¡å¼çš„æŒ‡é’ˆè°ƒæ•´ç”Ÿæˆå™¨
        self.pointer_adjustment_generator = nn.Sequential(
            nn.Linear(4 + d, d // 2),  # æ¨¡å¼ç‰¹å¾ + å½“å‰éšçŠ¶æ€
            nn.GELU(),  
            nn.Linear(d // 2, 1)  # ç”ŸæˆæŒ‡é’ˆè°ƒæ•´å»ºè®®
        )
        
        # ğŸ§  å¯å­¦ä¹ çš„æ¨¡å¼æƒé‡
        self.self_loop_weight = nn.Parameter(torch.tensor(0.1))      # è‡ªå¾ªç¯æ¨¡å¼æƒé‡
        self.short_jump_weight = nn.Parameter(torch.tensor(0.3))     # çŸ­è·³æ¨¡å¼æƒé‡  
        self.long_jump_weight = nn.Parameter(torch.tensor(0.4))      # é•¿è·³æ¨¡å¼æƒé‡
        self.convergence_weight = nn.Parameter(torch.tensor(0.2))    # æ”¶æ•›æ¨¡å¼æƒé‡
        
        # ğŸ”„ åæ€å¼ºåº¦æ§åˆ¶
        self.reflection_intensity = nn.Parameter(torch.tensor(0.2))
        
        # ğŸ¯ NEW: å¯å­¦ä¹ çš„è·ç¦»é˜ˆå€¼ (æ›¿ä»£ç¡¬ç¼–ç )
        self.short_jump_threshold = nn.Parameter(torch.tensor(2.0))   # çŸ­è·³è·ç¦»é˜ˆå€¼
        self.long_jump_ratio = nn.Parameter(torch.tensor(0.25))       # é•¿è·³è·ç¦»æ¯”ä¾‹ (ç›¸å¯¹äºåºåˆ—é•¿åº¦)
        
        # ğŸ§® å¯å­¦ä¹ çš„æ¨¡å¼è®¡ç®—æƒé‡
        self.pattern_consistency_weight = nn.Parameter(torch.tensor(1.0))    # ä¸€è‡´æ€§æƒé‡
        self.pattern_diversity_weight = nn.Parameter(torch.tensor(0.5))      # å¤šæ ·æ€§æƒé‡
        
        print(f"PointerChainReflection initialized: max_history={max_history_layers}, pattern_dim={pattern_analysis_dim}, learnable_thresholds=True")
    
    def _analyze_pointer_patterns_vectorized(self, pointer_history: List[torch.Tensor]) -> torch.Tensor:
        """
        å‘é‡åŒ–åˆ†ææŒ‡é’ˆé“¾ä¸­çš„æ¨¡å¼ - ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰ä½ç½®
        
        Args:
            pointer_history: å†å²æŒ‡é’ˆåˆ—è¡¨ [Tensor[B, N], ...]
            
        Returns:
            pattern_features: æ¨¡å¼ç‰¹å¾ [B, N, 4] (è‡ªå¾ªç¯ã€çŸ­è·³ã€é•¿è·³ã€æ”¶æ•›)
        """
        if not pointer_history:
            # å¦‚æœæ²¡æœ‰å†å²ï¼Œè¿”å›é›¶æ¨¡å¼
            return torch.zeros(1, 1, 4, device=torch.device('cpu'))
        
        B, N = pointer_history[0].shape
        device = pointer_history[0].device
        
        # æå–å†å²æŒ‡é’ˆçŸ©é˜µ [B, N, L]
        history_matrix = []
        for ptr_tensor in pointer_history[-self.max_history_layers:]:
            if ptr_tensor.size(1) == N:
                history_matrix.append(ptr_tensor)
            else:
                # å¤„ç†å°ºå¯¸ä¸åŒ¹é…çš„æƒ…å†µ
                padded = torch.zeros(B, N, device=device, dtype=torch.long)
                min_len = min(N, ptr_tensor.size(1))
                padded[:, :min_len] = ptr_tensor[:, :min_len]
                history_matrix.append(padded)
        
        if not history_matrix:
            return torch.zeros(B, N, 4, device=device)
        
        history_matrix = torch.stack(history_matrix, dim=2)  # [B, N, L]
        B, N, L = history_matrix.shape
        
        # ğŸš€ å‘é‡åŒ–æ¨¡å¼åˆ†æ
        patterns = torch.zeros(B, N, 4, device=device)
        
        # åˆ›å»ºä½ç½®ç´¢å¼•çŸ©é˜µ [B, N]
        position_indices = torch.arange(N, device=device).unsqueeze(0).expand(B, N)  # [B, N]
        
        # 1. è‡ªå¾ªç¯æ¨¡å¼ï¼šå‘é‡åŒ–è®¡ç®—æ‰€æœ‰ä½ç½®çš„è‡ªå¾ªç¯é¢‘ç‡
        self_loops = (history_matrix == position_indices.unsqueeze(2)).float().mean(dim=2)  # [B, N]
        patterns[:, :, 0] = self_loops
        
        # 2. çŸ­è·³æ¨¡å¼ï¼šå‘é‡åŒ–è·ç¦»è®¡ç®—
        short_threshold = torch.relu(self.short_jump_threshold)
        distances = torch.abs(history_matrix.float() - position_indices.unsqueeze(2).float())  # [B, N, L]
        is_short_jump = (distances <= short_threshold) & (history_matrix != position_indices.unsqueeze(2))
        short_jumps = is_short_jump.float().mean(dim=2)  # [B, N]
        patterns[:, :, 1] = short_jumps
        
        # 3. é•¿è·³æ¨¡å¼ï¼šå‘é‡åŒ–ç›¸å¯¹é˜ˆå€¼è®¡ç®—
        long_threshold = N * torch.sigmoid(self.long_jump_ratio)
        is_long_jump = distances > long_threshold
        long_jumps = is_long_jump.float().mean(dim=2)  # [B, N]
        patterns[:, :, 2] = long_jumps
        
        # 4. æ”¶æ•›æ¨¡å¼ï¼šå‘é‡åŒ–æ–¹å·®è®¡ç®—
        if L > 1:
            # è®¡ç®—æ¯ä¸ªä½ç½®å†å²çš„æ–¹å·® [B, N]
            variance = torch.var(history_matrix.float(), dim=2)  # [B, N]
            consistency_factor = torch.sigmoid(self.pattern_consistency_weight)
            diversity_factor = torch.sigmoid(self.pattern_diversity_weight)
            convergence = consistency_factor / (1.0 + diversity_factor * variance)
            patterns[:, :, 3] = convergence
        else:
            patterns[:, :, 3] = 0.5  # ä¸­æ€§å€¼
        
        return patterns
    
    def _compute_pointer_stability(self, pointer_history: List[torch.Tensor]) -> torch.Tensor:
        """
        è®¡ç®—æŒ‡é’ˆé“¾çš„ç¨³å®šæ€§ - è¡¡é‡æŒ‡é’ˆé€‰æ‹©çš„ä¸€è‡´æ€§ (ä½¿ç”¨å¯å­¦ä¹ å‚æ•°)
        
        Args:
            pointer_history: å†å²æŒ‡é’ˆåˆ—è¡¨
            
        Returns:
            stability: ç¨³å®šæ€§åˆ†æ•° [B, N]
        """
        if len(pointer_history) < 2:
            B, N = pointer_history[0].shape if pointer_history else (1, 1)
            device = pointer_history[0].device if pointer_history else torch.device('cpu')
            return torch.ones(B, N, device=device) * 0.5  # ä¸­æ€§ç¨³å®šæ€§
        
        # å¯å­¦ä¹ çš„ç¨³å®šæ€§çª—å£å¤§å°
        window_size = max(2, min(len(pointer_history), int(torch.relu(self.pattern_consistency_weight) * 3) + 2))
        recent_history = pointer_history[-window_size:]  # å–æœ€è¿‘å‡ å±‚
        consistency_scores = []
        
        for i in range(len(recent_history) - 1):
            # è®¡ç®—ç›¸é‚»å±‚æŒ‡é’ˆçš„ç›¸ä¼¼åº¦
            ptr1, ptr2 = recent_history[i], recent_history[i + 1]
            consistency = (ptr1 == ptr2).float()  # [B, N]
            consistency_scores.append(consistency)
        
        if consistency_scores:
            # ä½¿ç”¨å¯å­¦ä¹ æƒé‡è®¡ç®—ç¨³å®šæ€§
            weights = torch.softmax(torch.tensor([torch.sigmoid(self.pattern_diversity_weight)] * len(consistency_scores)), dim=0)
            stability = sum(w * score for w, score in zip(weights, consistency_scores))  # [B, N]
        else:
            B, N = recent_history[0].shape
            device = recent_history[0].device
            stability = torch.ones(B, N, device=device) * 0.5
        
        return stability
    
    def forward(self, h: torch.Tensor, pointer_history: List[torch.Tensor]) -> torch.Tensor:
        """
        åŸºäºæŒ‡é’ˆé“¾å†å²ç”Ÿæˆåæ€å¢å¼ºçš„ç‰¹å¾ - å®Œå…¨å‘é‡åŒ–ç‰ˆæœ¬
        
        Args:
            h: å½“å‰éšçŠ¶æ€ [B, N, d]
            pointer_history: æŒ‡é’ˆå†å² [Tensor[B, N], ...]
            
        Returns:
            reflected_h: åæ€å¢å¼ºçš„éšçŠ¶æ€ [B, N, d]
        """
        if not pointer_history:
            return h
        
        B, N, d = h.shape
        device = h.device
        
        # ğŸš€ å‘é‡åŒ–æ¨¡å¼åˆ†æ - ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰ä½ç½®
        all_patterns = self._analyze_pointer_patterns_vectorized(pointer_history)  # [B, N, 4]
        
        # ğŸ¯ å‘é‡åŒ–æŒ‡é’ˆè°ƒæ•´ç”Ÿæˆ
        # å°†éšçŠ¶æ€å’Œæ¨¡å¼ç‰¹å¾ç»„åˆ [B, N, d+4]
        combined_input = torch.cat([all_patterns, h], dim=-1)  # [B, N, 4+d]
        
        # æ‰¹é‡é€šè¿‡è°ƒæ•´ç”Ÿæˆå™¨
        adjustments = self.pointer_adjustment_generator(combined_input)  # [B, N, 1]
        
        # ğŸ§  å‘é‡åŒ–æ¨¡å¼æƒé‡è®¡ç®—
        pattern_weights = torch.stack([
            self.self_loop_weight,
            self.short_jump_weight, 
            self.long_jump_weight,
            self.convergence_weight
        ], dim=0)  # [4]
        
        # è®¡ç®—åŠ æƒæ¨¡å¼åˆ†æ•° [B, N, 1]
        weighted_patterns = all_patterns * pattern_weights.view(1, 1, 4)  # [B, N, 4]
        pattern_influence = weighted_patterns.sum(dim=-1, keepdim=True)  # [B, N, 1]
        
        # ğŸ”„ åŠ¨æ€åæ€å¼ºåº¦
        dynamic_intensity = torch.sigmoid(self.reflection_intensity) * pattern_influence
        
        # æœ€ç»ˆåæ€å¢å¼º - å‘é‡åŒ–è®¡ç®—
        h_norm = h.norm(dim=-1, keepdim=True)  # [B, N, 1]
        reflection_delta = dynamic_intensity * adjustments * h_norm
        reflected_h = h + reflection_delta
        
        return reflected_h


class PointerLayer(nn.Module):
    """DeepSeek-style Pointer Layer with Pointer-Chain Reflection Mechanism.
    
    Architecture: RMSNorm â†’ PointerBlock â†’ Residual â†’ RMSNorm â†’ SwiGLU â†’ Residual (Pre-norm)
    Key features: 
    - Passes prev_index to form pointer chains across layers
    - NEW: Pointer-chain based reflection for efficient and interpretable reasoning
    - Lightweight O(L*N) reflection instead of O(L*N*d) 
    
    Args:
        d (int): Hidden dimension
        n_heads (int): Number of attention heads
        layer_idx (int): Current layer index
        n_kv_heads (int): Number of key-value heads
        d_ff (int): Feed-forward hidden dimension (if None, auto-calculated)
        dropout (float): Dropout rate
        use_value_proj (bool): Whether to use value projection in PointerBlock
        use_alibi (bool): Whether to use AliBi positional encoding
        max_seq_len (int): Maximum sequence length
        reflection_config (dict): Reflection configuration parameters
    """
    
    def __init__(self, d, n_heads, layer_idx=0, n_kv_heads=None, d_ff=None, dropout=0.0,
                 use_value_proj=True, use_alibi=True, max_seq_len=4096, reflection_config=None):
        super().__init__()
        self.d = d
        self.layer_idx = layer_idx
        
        # ğŸ§  NEW: åŸºäºæŒ‡é’ˆé“¾çš„åæ€æœºåˆ¶
        self.reflection_config = reflection_config or {}
        max_history = self.reflection_config.get('max_history_layers', 8)
        self.pointer_chain_reflection = PointerChainReflection(
            d=d,
            max_history_layers=max_history,
            pattern_analysis_dim=64
        )
        
        # DeepSeek-style RMSNorm (Pre-norm architecture)
        self.norm1 = RMSNorm(d)
        self.norm2 = RMSNorm(d)
        
        # Pointer block
        self.pointer_block = PointerBlock(
            d=d,
            n_heads=n_heads,
            n_kv_heads=n_kv_heads,
            use_value_proj=use_value_proj,
            use_alibi=use_alibi,
            max_seq_len=max_seq_len,
        )
        
        # Learnable gate for pointer output
        self.gate = nn.Parameter(torch.ones(d))
        
        # Llama-style SwiGLU FFN
        self.ffn = LlamaMLP(
            hidden_size=d,
            intermediate_size=d_ff or int(8 * d / 3),
            hidden_act="silu"
        )
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
        print(f"PointerLayer {layer_idx} initialized: d={d}, n_heads={n_heads}, "
              f"pointer_chain_reflection=True, max_history={max_history}")
    
    def forward(self, h, kv_cache=None, prev_idx=None, layer_history=None, pointer_history=None, return_full_scores=False):
        """DeepSeek-style forward pass with pointer-chain reflection.
        
        Args:
            h (torch.Tensor): Input hidden states [B, N, d]
            kv_cache (Optional): KV cache for inference
            prev_idx (Optional[torch.Tensor]): Previous layer's pointer indices for chaining
            layer_history (Optional[List[torch.Tensor]]): DEPRECATED - not used in new reflection
            pointer_history (Optional[List[torch.Tensor]]): Pointer indices history for reflection
            return_full_scores (bool): Whether to return full position scores
            
        Returns:
            Tuple containing:
                - h (torch.Tensor): Output hidden states [B, N, d]
                - idx (torch.Tensor): Current layer's pointer indices [B, N] 
                - p (torch.Tensor): Pointer probabilities [B, N]
                - full_scores (Optional[torch.Tensor]): Full position scores if requested
        """
        # ğŸ§  NEW: åŸºäºæŒ‡é’ˆé“¾çš„è½»é‡çº§åæ€
        if pointer_history:
            h = self.pointer_chain_reflection(h, pointer_history)
        
        # --- Pointer part (Pre-norm) ---
        residual = h
        
        # Pre-norm: normalize then compute
        h_norm = self.norm1(h)
        
        # Pointer computation
        pointer_result = self.pointer_block(h_norm, kv_cache, prev_idx=prev_idx, return_full_scores=return_full_scores)
        
        # Always ensure we have the right number of values
        if return_full_scores:
            if len(pointer_result) == 4:
                z, idx, p, full_scores = pointer_result
            else:
                z, idx, p = pointer_result
                # Create dummy full_scores for compatibility
                B, N = h.shape[:2]
                N_cache = kv_cache.max_seq_len if (kv_cache and hasattr(kv_cache, 'max_seq_len')) else self.pointer_block.max_seq_len
                full_scores = torch.zeros(B, N, N_cache, device=h.device)
        else:
            if len(pointer_result) == 4:
                z, idx, p, _ = pointer_result  # Ignore full_scores
            else:
                z, idx, p = pointer_result
        
        # Apply learnable gate and residual connection
        z = z * self.gate
        h = residual + self.dropout(z)
        
        # --- FFN part (Pre-norm) ---
        residual = h
        h = self.norm2(h)
        h = self.ffn(h)
        h = residual + self.dropout(h)
        
        if return_full_scores:
            return h, idx, p, full_scores
        else:
            return h, idx, p
