#!/usr/bin/env python3
"""
æµ‹è¯•çº¯å…³ç³»å»ºæ¨¡çš„æ•ˆæœ
éªŒè¯æ–°çš„PointerBlockèƒ½å¦æœ‰æ•ˆæ›¿ä»£å…¨å±€æ³¨æ„åŠ›æœºåˆ¶
"""

import torch
import torch.nn as nn
import sys
import os

# Add src path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.layers.pointer_block import PointerBlock
from src.layers.pointer_layer import PointerLayer
from src.model.pointer_model import PointerDecoder


def test_relation_modeling():
    """æµ‹è¯•çº¯å…³ç³»å»ºæ¨¡çš„åŸºæœ¬åŠŸèƒ½"""
    print("ğŸ¯ æµ‹è¯•çº¯å…³ç³»å»ºæ¨¡...")
    
    # é…ç½®
    B, N, d = 2, 8, 64
    n_heads = 8
    device = torch.device('cpu')
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    h = torch.randn(B, N, d, device=device)
    prev_idx = torch.randint(0, N, (B, N), device=device)
    
    # åˆ›å»ºPointerBlock
    pointer_block = PointerBlock(
        d=d, 
        n_heads=n_heads,
        use_alibi=False,  # å…³é—­ALiBi
        max_seq_len=N
    )
    
    print(f"è¾“å…¥å½¢çŠ¶: {h.shape}")
    print(f"å‰å±‚å…³ç³»é“¾: {prev_idx.shape}")
    
    # å‰å‘ä¼ æ’­
    z, relation_targets, relation_strength = pointer_block(h, prev_idx=prev_idx)
    
    print(f"è¾“å‡ºå½¢çŠ¶: {z.shape}")
    print(f"å…³ç³»ç›®æ ‡: {relation_targets.shape}")
    print(f"å…³ç³»å¼ºåº¦: {relation_strength.shape}")
    
    # éªŒè¯å…³ç³»é“¾ä¼ é€’
    print("\nğŸ”— å…³ç³»é“¾åˆ†æ:")
    for b in range(B):
        print(f"æ‰¹æ¬¡ {b}:")
        print(f"  å‰å±‚é“¾: {prev_idx[b].tolist()}")
        print(f"  å½“å‰é“¾: {relation_targets[b].tolist()}")
        print(f"  å¼ºåº¦: {relation_strength[b].tolist()}")
    
    return True


def test_reflection_mechanism():
    """æµ‹è¯•åæ€æœºåˆ¶æ˜¯å¦èƒ½åˆ©ç”¨å…³ç³»é“¾"""
    print("\nğŸ§  æµ‹è¯•åæ€æœºåˆ¶...")
    
    # é…ç½®
    B, N, d = 1, 6, 32
    n_heads = 4
    device = torch.device('cpu')
    
    # åæ€é…ç½®
    reflection_config = {
        'reflection_layers': [2],  # ç¬¬2å±‚å¯ç”¨åæ€
        'pointer_backtrack_layers': 3,
        'reflection_gate_init': 0.2
    }
    
    # åˆ›å»ºåæ€å±‚
    reflection_layer = PointerLayer(
        d=d,
        n_heads=n_heads,
        layer_idx=2,  # åæ€å±‚
        reflection_config=reflection_config
    )
    
    # æ¨¡æ‹Ÿå†å²æ•°æ®
    h = torch.randn(B, N, d, device=device)
    layer_history = [torch.randn(B, N, d, device=device) for _ in range(3)]
    pointer_history = [torch.randint(0, N, (B, N), device=device) for _ in range(3)]
    
    print(f"è¾“å…¥å½¢çŠ¶: {h.shape}")
    print(f"å†å²å±‚æ•°: {len(layer_history)}")
    print(f"å†å²å…³ç³»æ•°: {len(pointer_history)}")
    
    # å‰å‘ä¼ æ’­
    h_out, relation_targets, relation_strength = reflection_layer(
        h, 
        layer_history=layer_history,
        pointer_history=pointer_history
    )
    
    print(f"è¾“å‡ºå½¢çŠ¶: {h_out.shape}")
    print(f"å…³ç³»ç›®æ ‡: {relation_targets}")
    print(f"å…³ç³»å¼ºåº¦å‡å€¼: {relation_strength.mean().item():.3f}")
    
    # æ£€æŸ¥åæ€ç‰¹å¾
    if hasattr(reflection_layer, 'last_reflection_features') and reflection_layer.last_reflection_features is not None:
        print(f"åæ€ç‰¹å¾å½¢çŠ¶: {reflection_layer.last_reflection_features.shape}")
        print("âœ… åæ€æœºåˆ¶æ­£å¸¸å·¥ä½œ")
    else:
        print("âŒ åæ€æœºåˆ¶æœªæ¿€æ´»")
    
    return True


def test_model_integration():
    """æµ‹è¯•å®Œæ•´æ¨¡å‹çš„é›†æˆ"""
    print("\nğŸš€ æµ‹è¯•å®Œæ•´æ¨¡å‹é›†æˆ...")
    
    # å°å‹æ¨¡å‹é…ç½®
    config = {
        'vocab_size': 100,
        'd': 64,
        'n_layers': 4,
        'n_heads': 8,
        'top_k': 1,  # çº¯å…³ç³»å»ºæ¨¡ï¼Œæ¯ä¸ªtokenæŒ‡å‘1ä¸ªç›®æ ‡
        'max_seq_len': 16,
        'reflection_config': {
            'reflection_layers': [2, 3],  # åä¸¤å±‚å¯ç”¨åæ€
            'pointer_backtrack_layers': 2,
            'reflection_gate_init': 0.1
        }
    }
    
    # åˆ›å»ºæ¨¡å‹
    model = PointerDecoder(**config)
    model.eval()
    
    # æµ‹è¯•æ•°æ®
    B, N = 2, 12
    input_ids = torch.randint(0, config['vocab_size'], (B, N))
    
    print(f"è¾“å…¥åºåˆ—å½¢çŠ¶: {input_ids.shape}")
    
    # å‰å‘ä¼ æ’­ - éœ€è¦è®­ç»ƒæ¨¡å¼æˆ–ä½¿ç”¨cacheæ¥è·å–pointer_indices
    model.train()  # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼ä»¥è·å–pointer_indices
    with torch.no_grad():
        result = model(input_ids, output_hiddens=True)
    
    print(f"è¾“å‡ºlogitså½¢çŠ¶: {result['logits'].shape}")
    print(f"æŒ‡é’ˆç´¢å¼•å±‚æ•°: {len(result['pointer_indices'])}")
    
    # åˆ†æå…³ç³»æ¨¡å¼
    print("\nğŸ“Š å…³ç³»æ¨¡å¼åˆ†æ:")
    if result['pointer_indices']:
        last_relations = result['pointer_indices'][-1]  # æœ€åä¸€å±‚çš„å…³ç³»
        print(f"æœ€åä¸€å±‚å…³ç³»: {last_relations[0].tolist()}")  # ç¬¬ä¸€ä¸ªæ ·æœ¬
        
        # è®¡ç®—å…³ç³»ç»Ÿè®¡
        stats = model.get_pointer_stats()
        if stats:
            print(f"å…³ç³»åˆ©ç”¨ç‡: {stats.get('pointer_utilization', 0):.3f}")
            print(f"å¹³å‡è·³è·ƒè·ç¦»: {stats.get('avg_hop_distance', 0):.3f}")
            print(f"å…³ç³»ç†µ: {stats.get('pointer_entropy', 0):.3f}")
    
    print("âœ… æ¨¡å‹é›†æˆæµ‹è¯•é€šè¿‡")
    return True


def benchmark_speed():
    """ç®€å•çš„é€Ÿåº¦åŸºå‡†æµ‹è¯•"""
    print("\nâš¡ é€Ÿåº¦åŸºå‡†æµ‹è¯•...")
    
    # é…ç½®
    B, N, d = 4, 32, 128
    n_heads = 8
    device = torch.device('cpu')
    
    # åˆ›å»ºä¼ ç»Ÿæ³¨æ„åŠ›å¯¹æ¯”
    class SimpleAttention(nn.Module):
        def __init__(self, d, n_heads):
            super().__init__()
            self.d = d
            self.n_heads = n_heads
            self.head_dim = d // n_heads
            self.q_proj = nn.Linear(d, d)
            self.k_proj = nn.Linear(d, d)
            self.v_proj = nn.Linear(d, d)
            self.o_proj = nn.Linear(d, d)
            
        def forward(self, h):
            B, N, d = h.shape
            q = self.q_proj(h).view(B, N, self.n_heads, self.head_dim).transpose(1, 2)
            k = self.k_proj(h).view(B, N, self.n_heads, self.head_dim).transpose(1, 2)
            v = self.v_proj(h).view(B, N, self.n_heads, self.head_dim).transpose(1, 2)
            
            scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
            attn = torch.softmax(scores, dim=-1)
            out = torch.matmul(attn, v)
            out = out.transpose(1, 2).contiguous().view(B, N, d)
            return self.o_proj(out)
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å—
    attention = SimpleAttention(d, n_heads)
    pointer_block = PointerBlock(d, n_heads, use_alibi=False)
    
    # æµ‹è¯•æ•°æ®
    h = torch.randn(B, N, d, device=device)
    
    # Warmup
    for _ in range(5):
        _ = attention(h)
        _ = pointer_block(h)
    
    import time
    
    # æµ‹è¯•ä¼ ç»Ÿæ³¨æ„åŠ›
    start_time = time.time()
    for _ in range(100):
        _ = attention(h)
    attention_time = time.time() - start_time
    
    # æµ‹è¯•çº¯å…³ç³»å»ºæ¨¡
    start_time = time.time()
    for _ in range(100):
        _ = pointer_block(h)
    relation_time = time.time() - start_time
    
    print(f"ä¼ ç»Ÿæ³¨æ„åŠ›æ—¶é—´: {attention_time:.3f}s")
    print(f"çº¯å…³ç³»å»ºæ¨¡æ—¶é—´: {relation_time:.3f}s")
    print(f"é€Ÿåº¦æå‡: {attention_time / relation_time:.2f}x")
    
    return True


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ çº¯å…³ç³»å»ºæ¨¡æµ‹è¯•å¥—ä»¶")
    print("=" * 50)
    
    try:
        # åŸºç¡€åŠŸèƒ½æµ‹è¯•
        test_relation_modeling()
        
        # åæ€æœºåˆ¶æµ‹è¯•
        test_reflection_mechanism()
        
        # æ¨¡å‹é›†æˆæµ‹è¯•
        test_model_integration()
        
        # é€Ÿåº¦åŸºå‡†æµ‹è¯•
        benchmark_speed()
        
        print("\n" + "=" * 50)
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("ğŸ‰ çº¯å…³ç³»å»ºæ¨¡å·²æˆåŠŸæ›¿ä»£å…¨å±€æ³¨æ„åŠ›æœºåˆ¶")
        print("ğŸš€ å…³ç³»é“¾æ„å»ºæ˜¾å¼æ€ç»´é“¾ï¼Œåæ€é—¨æ§æœ‰æ•ˆåˆ©ç”¨å†å²å…³ç³»")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


if __name__ == "__main__":
    main()