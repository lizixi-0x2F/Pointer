#!/usr/bin/env python3
"""
æ ‡å‡†åŸºå‡†æµ‹è¯•ï¼šListOps-32 & WikiText-2
ä½¿ç”¨ä¸šç•Œæ ‡å‡†æ•°æ®é›†è¿›è¡Œä¸¥è°¨çš„æ¨¡å‹å¯¹æ¯”

å¯¹æ¯”çº¯å…³ç³»å»ºæ¨¡ vs ä¼ ç»Ÿæ¶æ„ï¼š
- ListOps-32: ç»“æ„åŒ–æ¨ç†ä»»åŠ¡  
- WikiText-2: è¯­è¨€å»ºæ¨¡ä»»åŠ¡
"""

import torch
import time
import numpy as np
import argparse
import sys
import os
import json
from typing import Dict, List, Optional
import matplotlib.pyplot as plt

# Add project paths
sys.path.append('/Volumes/oz/pointer')
sys.path.append('/Volumes/oz/pointer/benchmarks')

from standard_datasets import create_listops_dataloaders, create_wikitext_dataloaders
from fair_config import get_fair_training_config, print_fair_config, FAIR_HYPERPARAMS

# Import model trainers
from simple_trainers import (
    PointerTaskTrainer, TransformerTaskTrainer, MambaTaskTrainer, 
    PerformerTaskTrainer, TrainingConfig
)


def create_standard_config(task: str, model_size: str = "small", max_steps: int = None, enable_reflection: bool = False):
    """åˆ›å»ºæ ‡å‡†åŸºå‡†é…ç½®"""
    
    # ğŸ¯ é’ˆå¯¹æ ‡å‡†ä»»åŠ¡ä¼˜åŒ–çš„é…ç½®
    if model_size == "tiny":
        base_config = {
            'd_model': 256,
            'n_layers': 4,
            'n_heads': 8,
            'max_seq_len': 512,
            'batch_size': 16,
            'max_steps': max_steps or 3000,
            'eval_interval': 300,
        }
    elif model_size == "small":
        base_config = {
            'd_model': 384,
            'n_layers': 6, 
            'n_heads': 12,
            'max_seq_len': 512,
            'batch_size': 12,
            'max_steps': max_steps or 5000,
            'eval_interval': 200,
        }
    elif model_size == "medium":
        base_config = {
            'd_model': 512,
            'n_layers': 8,
            'n_heads': 16,
            'max_seq_len': 512,
            'batch_size': 8,
            'max_steps': max_steps or 8000,
            'eval_interval': 800,
        }
    else:
        raise ValueError(f"Unknown model size: {model_size}")
    
    # æ ¹æ®ä»»åŠ¡è°ƒæ•´é…ç½®
    if task == "listops":
        # ListOpséœ€è¦æ›´å¼ºçš„ç»“æ„åŒ–æ¨ç†èƒ½åŠ›
        base_config['top_k'] = 1  # çº¯å…³ç³»å»ºæ¨¡
        base_config['n_kv_heads'] = base_config['n_heads']  # å……åˆ†çš„å…³ç³»å»ºæ¨¡èƒ½åŠ›
        vocab_size = 50  # ListOpsè¯æ±‡è¡¨è¾ƒå°
    elif task == "wikitext":
        # WikiTextéœ€è¦æ›´å¼ºçš„è¯­è¨€å»ºæ¨¡èƒ½åŠ›
        base_config['top_k'] = 1  # çº¯å…³ç³»å»ºæ¨¡
        base_config['n_kv_heads'] = base_config['n_heads']
        vocab_size = 70000  # WikiTextè¯æ±‡è¡¨è¾ƒå¤§
    else:
        raise ValueError(f"Unknown task: {task}")
    
    # ç³»ç»Ÿé…ç½®
    device = 'cpu'  # Use CPU to avoid MPS compatibility issues
    fp16 = torch.cuda.is_available()
    
    config = TrainingConfig(
        vocab_size=vocab_size,
        **base_config,
        device=device,
        fp16=fp16,
        grad_checkpoint=False,
        output_dir=f'./standard_benchmark_{task}',
        experiment_name=f'standard_{task}_benchmark'
    )
    
    # åº”ç”¨å…¬å¹³é…ç½®
    config = get_fair_training_config(config)
    
    # åæ€æœºåˆ¶é…ç½®
    if enable_reflection:
        reflection_layers = list(range(max(1, base_config['n_layers'] // 2), base_config['n_layers']))
        config.reflection_config = {
            'reflection_layers': reflection_layers,
            'pointer_backtrack_layers': 3,
            'reflection_gate_init': 0.1
        }
    else:
        config.reflection_config = {
            'reflection_layers': [],
            'pointer_backtrack_layers': 0,
            'reflection_gate_init': 0.0
        }
    
    return config


class StandardTaskTrainer:
    """æ ‡å‡†ä»»åŠ¡çš„é€šç”¨è®­ç»ƒå™¨"""
    
    def __init__(self, model_trainer, task_type: str):
        self.trainer = model_trainer
        self.task_type = task_type
        
    def train(self, train_loader, val_loader):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"å¼€å§‹è®­ç»ƒ {self.task_type} ä»»åŠ¡...")
        
        self.trainer.train_step = 0
        best_metric = float('inf') if self.task_type == 'language_modeling' else 0.0
        
        for epoch in range(100):  # æœ€å¤§epochæ•°
            epoch_loss = 0.0
            num_batches = 0
            
            self.trainer.model.train()
            for batch in train_loader:
                if self.trainer.train_step >= self.trainer.config.max_steps:
                    break
                
                # å‡†å¤‡è¾“å…¥æ•°æ®
                input_ids = batch['input_ids'].to(self.trainer.device)
                labels = batch['labels'].to(self.trainer.device)
                attention_mask = batch.get('attention_mask')
                
                # å‰å‘ä¼ æ’­
                if hasattr(self.trainer.model, 'forward'):
                    outputs = self.trainer.model(input_ids=input_ids, labels=labels)
                    loss = outputs['loss']
                else:
                    # å…¼å®¹ä¸åŒçš„æ¨¡å‹æ¥å£
                    loss = self.trainer.model(input_ids, labels=labels)
                
                # åå‘ä¼ æ’­
                self.trainer.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.trainer.model.parameters(), self.trainer.config.grad_clip_norm)
                self.trainer.optimizer.step()
                self.trainer.scheduler.step()
                
                epoch_loss += loss.item()
                num_batches += 1
                self.trainer.train_step += 1
                
                # å®šæœŸè¯„ä¼°
                if self.trainer.train_step % self.trainer.config.eval_interval == 0:
                    val_metrics = self.evaluate(val_loader)
                    
                    print(f"Step {self.trainer.train_step}:")
                    print(f"  Train Loss: {loss.item():.4f}")
                    
                    if self.task_type == 'classification':
                        print(f"  Val Accuracy: {val_metrics['accuracy']:.4f}")
                        print(f"  Val Loss: {val_metrics['loss']:.4f}")
                        
                        # ä¿å­˜æœ€ä½³æ¨¡å‹
                        if val_metrics['accuracy'] > best_metric:
                            best_metric = val_metrics['accuracy']
                            self.trainer.save_checkpoint('best_model.pt')
                    else:  # language_modeling
                        perplexity = np.exp(val_metrics['loss'])
                        print(f"  Val Loss: {val_metrics['loss']:.4f}")
                        print(f"  Val Perplexity: {perplexity:.2f}")
                        
                        # ä¿å­˜æœ€ä½³æ¨¡å‹ (æ›´ä½çš„lossæ›´å¥½)
                        if val_metrics['loss'] < best_metric:
                            best_metric = val_metrics['loss']
                            self.trainer.save_checkpoint('best_model.pt')
            
            if self.trainer.train_step >= self.trainer.config.max_steps:
                break
        
        print(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³æŒ‡æ ‡: {best_metric:.4f}")
    
    def evaluate(self, data_loader):
        """è¯„ä¼°æ¨¡å‹"""
        self.trainer.model.eval()
        total_loss = 0.0
        total_correct = 0
        total_samples = 0
        
        with torch.no_grad():
            for batch in data_loader:
                input_ids = batch['input_ids'].to(self.trainer.device)
                labels = batch['labels'].to(self.trainer.device)
                
                # å‰å‘ä¼ æ’­
                if hasattr(self.trainer.model, 'forward'):
                    outputs = self.trainer.model(input_ids=input_ids, labels=labels)
                    loss = outputs['loss']
                    logits = outputs['logits']
                else:
                    # å…¼å®¹å¤„ç†
                    result = self.trainer.model(input_ids, labels=labels)
                    loss = result if isinstance(result, torch.Tensor) else result.get('loss', result)
                    logits = getattr(result, 'logits', None)
                
                total_loss += loss.item()
                
                # è®¡ç®—å‡†ç¡®ç‡ (ä»…åˆ†ç±»ä»»åŠ¡)
                if self.task_type == 'classification' and logits is not None:
                    preds = torch.argmax(logits, dim=-1)
                    total_correct += (preds == labels).sum().item()
                    total_samples += labels.size(0)
        
        avg_loss = total_loss / len(data_loader)
        metrics = {'loss': avg_loss}
        
        if self.task_type == 'classification' and total_samples > 0:
            metrics['accuracy'] = total_correct / total_samples
        
        return metrics


def train_model_on_standard_task(model_type: str, task: str, config: TrainingConfig):
    """åœ¨æ ‡å‡†ä»»åŠ¡ä¸Šè®­ç»ƒæŒ‡å®šæ¨¡å‹"""
    print(f"\nğŸš€ Training {model_type.upper()} on {task.upper()}...")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    if task == "listops":
        data_loaders = create_listops_dataloaders('/Volumes/oz/pointer/listops-32', 
                                                 batch_size=config.batch_size, 
                                                 max_seq_len=config.max_seq_len)
        task_type = 'classification'
    elif task == "wikitext":
        data_loaders = create_wikitext_dataloaders('/Volumes/oz/pointer/wikitext-2',
                                                  batch_size=config.batch_size,
                                                  max_seq_len=config.max_seq_len)
        task_type = 'language_modeling'
    else:
        raise ValueError(f"Unknown task: {task}")
    
    # æ›´æ–°é…ç½®ä¸­çš„è¯æ±‡è¡¨å¤§å°
    config.vocab_size = data_loaders['vocab_size']
    
    # åˆ›å»ºå¯¹åº”çš„è®­ç»ƒå™¨
    if model_type == "pointer":
        trainer = PointerTaskTrainer(config, f"{task}_{model_type}")
    elif model_type == "transformer":
        trainer = TransformerTaskTrainer(config, f"{task}_{model_type}")
    elif model_type == "mamba":
        trainer = MambaTaskTrainer(config, f"{task}_{model_type}")
    elif model_type == "performer":
        trainer = PerformerTaskTrainer(config, f"{task}_{model_type}")
    else:
        raise ValueError(f"Unknown model type: {model_type}")
    
    # åŒ…è£…ä¸ºæ ‡å‡†ä»»åŠ¡è®­ç»ƒå™¨
    standard_trainer = StandardTaskTrainer(trainer, task_type)
    
    # è®­ç»ƒæ¨¡å‹
    start_time = time.time()
    try:
        standard_trainer.train(data_loaders['train'], data_loaders['validation'])
        train_time = time.time() - start_time
        success = True
    except Exception as e:
        print(f"âŒ Training failed: {e}")
        import traceback
        traceback.print_exc()
        train_time = time.time() - start_time
        success = False
        return None
    
    if not success:
        return None
    
    print(f"âœ… {model_type} on {task} completed in {train_time:.1f}s")
    
    # è¯„ä¼°æµ‹è¯•é›†
    test_metrics = standard_trainer.evaluate(data_loaders['test'])
    
    # ğŸ¯ å…³ç³»è´¨é‡åˆ†æ (ä»…å¯¹Pointeræ¨¡å‹)
    relation_metrics = {}
    if model_type == "pointer" and hasattr(trainer.model, 'get_pointer_stats'):
        relation_stats = trainer.model.get_pointer_stats()
        relation_metrics = {
            'relation_utilization': relation_stats.get('pointer_utilization', 0),
            'avg_hop_distance': relation_stats.get('avg_hop_distance', 0),
            'relation_entropy': relation_stats.get('pointer_entropy', 0)
        }
        
        print(f"ğŸ”— å…³ç³»è´¨é‡æŒ‡æ ‡:")
        for key, value in relation_metrics.items():
            print(f"   {key}: {value:.3f}")
    
    # è®¡ç®—æ¨¡å‹ç»Ÿè®¡
    model_params = sum(p.numel() for p in trainer.model.parameters()) / 1e6
    
    result = {
        'model_type': model_type,
        'task': task,
        'task_type': task_type,
        'train_time': train_time,
        'test_metrics': test_metrics,
        'relation_metrics': relation_metrics,
        'model_params': model_params,
        'vocab_size': data_loaders['vocab_size']
    }
    
    # æ·»åŠ ä¸»è¦æŒ‡æ ‡
    if task_type == 'classification':
        result['main_metric'] = test_metrics.get('accuracy', 0)
        result['metric_name'] = 'accuracy'
    else:
        result['main_metric'] = np.exp(test_metrics.get('loss', 10))  # perplexity
        result['metric_name'] = 'perplexity'
    
    return result


def main():
    parser = argparse.ArgumentParser(description='Standard Dataset Benchmark (ListOps-32 & WikiText-2)')
    parser.add_argument('--models', nargs='+',
                       choices=['pointer', 'transformer', 'mamba', 'performer'],
                       default=['pointer', 'transformer'],
                       help='Models to compare')
    parser.add_argument('--model-size', choices=['tiny', 'small', 'medium'], default='small',
                       help='Model size configuration')
    parser.add_argument('--tasks', nargs='+', 
                       choices=['listops', 'wikitext'],
                       default=['listops'],
                       help='Standard tasks to benchmark')
    parser.add_argument('--max-steps', type=int, help='Override max training steps')
    
    args = parser.parse_args()
    
    print("ğŸ¯ STANDARD DATASET BENCHMARK")
    print("=" * 80)
    print("ğŸ“Š Standard Tasks:", ", ".join(args.tasks))
    print("ğŸ¤– Models:", ", ".join(args.models))
    device_name = "MPS" if torch.backends.mps.is_available() else ("CUDA" if torch.cuda.is_available() else "CPU")
    print(f"ğŸ’» Device: {device_name}")
    print(f"ğŸ“ Model Size: {args.model_size}")
    print()
    
    # æ˜¾ç¤ºå…¬å¹³é…ç½®
    print_fair_config()
    print()
    
    # è¿è¡Œæ‰€æœ‰å®éªŒ
    results = []
    total_experiments = len(args.tasks) * len(args.models)
    experiment_count = 0
    
    for task in args.tasks:
        for model in args.models:
            experiment_count += 1
            print(f"\n{'='*60}")
            print(f"ğŸ”¬ Experiment {experiment_count}/{total_experiments}: {model.upper()} on {task.upper()}")
            print(f"{'='*60}")
            
            # åˆ›å»ºä»»åŠ¡ç‰¹å®šçš„é…ç½®
            config = create_standard_config(task, args.model_size, args.max_steps, False)
            
            try:
                result = train_model_on_standard_task(model, task, config)
                if result:
                    results.append(result)
                    metric_name = result['metric_name']
                    metric_value = result['main_metric']
                    print(f"âœ… Success: {metric_name} = {metric_value:.4f}")
                else:
                    print("âŒ Training failed")
            except Exception as e:
                print(f"âŒ Experiment failed: {e}")
                import traceback
                traceback.print_exc()
                continue
    
    if not results:
        print("âŒ No experiments completed successfully!")
        return
    
    print(f"\nğŸ¯ Completed {len(results)}/{total_experiments} experiments successfully")
    
    # æ‰“å°è¯¦ç»†ç»“æœ
    print("\n" + "="*100)
    print("ğŸ¯ STANDARD BENCHMARK RESULTS")
    print("="*100)
    
    for result in results:
        model = result['model_type'].upper()
        task = result['task'].upper() 
        metric = result['main_metric']
        metric_name = result['metric_name']
        time_val = result['train_time']
        params = result['model_params']
        
        print(f"{model:<12} on {task:<8} | {metric_name}: {metric:.4f} | Time: {time_val:6.1f}s | Params: {params:.1f}M")
        
        # æ˜¾ç¤ºå…³ç³»è´¨é‡æŒ‡æ ‡ï¼ˆä»…Pointeræ¨¡å‹ï¼‰
        if result['model_type'] == 'pointer' and result['relation_metrics']:
            rm = result['relation_metrics']
            print(f"             ğŸ”— Relations: Util={rm['relation_utilization']:.3f} | Hop={rm['avg_hop_distance']:.2f} | Entropy={rm['relation_entropy']:.3f}")
    
    print("="*100)


if __name__ == "__main__":
    main()